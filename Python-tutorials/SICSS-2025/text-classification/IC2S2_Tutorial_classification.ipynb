{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBB8uSgCWsQq"
      },
      "source": [
        "Colab notebook written by Emma Bonutti D'Agostini, Emilien Schultz, and Julien Boelaert, July 2025."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgo_gScVDuTR"
      },
      "source": [
        "# Initial setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qKr_1M5qXms",
        "outputId": "b804185d-6b59-44f3-cde0-57ffb2870f5b"
      },
      "outputs": [],
      "source": [
        "# Install modules\n",
        "!pip install -q pandas scikit-learn openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HYHsEEtLqac1"
      },
      "outputs": [],
      "source": [
        "# Load modules\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVBLvrTnqgYq"
      },
      "outputs": [],
      "source": [
        "# API, Key, and Model selection\n",
        "api_url = \"https://openrouter.ai/api/v1\"\n",
        "api_key = \"YOUR KEY\" # INSERT YOUR KEY HERE\n",
        "\n",
        "client = OpenAI(\n",
        "  base_url=api_url,\n",
        "  api_key=api_key,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "UEtx5jxBqdvs",
        "outputId": "996398f0-e35b-4b43-afc4-b9be87057f34"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>gold_standard</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gold-Winning Canadian Snowboarder Cops To Erro...</td>\n",
              "      <td>SPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Breaking: Israelites in Sinai Suddenly Achieve...</td>\n",
              "      <td>RELIGION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why Air Travel Still Sucks</td>\n",
              "      <td>TECH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7 Fashion And Beauty New Year's Resolutions To...</td>\n",
              "      <td>STYLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Panthers Owner To Treat Entire Staff To Free T...</td>\n",
              "      <td>SPORTS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headline gold_standard\n",
              "0  Gold-Winning Canadian Snowboarder Cops To Erro...        SPORTS\n",
              "1  Breaking: Israelites in Sinai Suddenly Achieve...      RELIGION\n",
              "2                         Why Air Travel Still Sucks          TECH\n",
              "3  7 Fashion And Beauty New Year's Resolutions To...         STYLE\n",
              "4  Panthers Owner To Treat Entire Staff To Free T...        SPORTS"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load data\n",
        "data_path = \"https://raw.githubusercontent.com/css-polytechnique/css-ipp-materials/refs/heads/main/Python-tutorials/SICSS-2025/text-classification/headlines.csv\"\n",
        "headlines = pd.read_csv(data_path)\n",
        "headlines.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "hHfbxoRW2FQk",
        "outputId": "6d3bed6a-dd1f-432e-d6cc-3029edc62e44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "gold_standard\n",
              "POLITICS       40\n",
              "SPORTS         17\n",
              "BUSINESS       11\n",
              "CRIME          10\n",
              "STYLE           8\n",
              "TECH            7\n",
              "ENVIRONMENT     4\n",
              "RELIGION        3\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "headlines['gold_standard'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "MIUCeCld4Xd3",
        "outputId": "decc2bb8-4861-484c-9d94-ee427766db6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "gold_politics\n",
              "OTHER       60\n",
              "POLITICS    40\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## We will also create a dichotomic POLITICS/OTHER variable\n",
        "headlines['gold_politics'] = headlines['gold_standard'].apply(\n",
        "    lambda x: 'POLITICS' if x == 'POLITICS' else 'OTHER')\n",
        "\n",
        "headlines['gold_politics'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xKzkUKae7nHU"
      },
      "outputs": [],
      "source": [
        "# Classification function\n",
        "# NB: this will require a prompt_generator function, defined below\n",
        "# You don't need to modify this function\n",
        "\n",
        "\n",
        "def get_predictions(prompt_generator, texts, model):\n",
        "  \"\"\"\n",
        "  Inference with the API for a model, a list of texts and a prompt format\n",
        "  \"\"\"\n",
        "  results = []\n",
        "  for i,j in texts.items():\n",
        "    try:\n",
        "      print(f\"\\rRequest element {i}\", end= \"\")\n",
        "      completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=prompt_generator(j)\n",
        "      )\n",
        "      results.append(completion)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      results.append(None)\n",
        "  print(\"\\rPrediction finished\")\n",
        "  return [i.choices[0].message.content for i in results]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6HUqTK8D1-F"
      },
      "source": [
        "# Zero-shot classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIMDfTu5KDhe"
      },
      "source": [
        "## Prompt engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORmPEDJDvgBI",
        "outputId": "e58e36c8-a872-4ea9-a481-19b4144698c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': \"You are a helpful and accurate news headline classifier. Your job is to classify news headlines as either 'POLITICS' or 'OTHER'. Only respond with exactly one of those two labels.\"},\n",
              " {'role': 'user',\n",
              "  'content': 'Classify this headline:\\n\"Gold-Winning Canadian Snowboarder Cops To Error That Wasn\\'t Spotted By Judges\"\\n'}]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Zero shot: prompt engineering\n",
        "\n",
        "## Function for prompt creation. You can modify the instructions here.\n",
        "### NB: this is a zero-shot prompt, we will cover few-shot later\n",
        "\n",
        "def build_prompt_basic(text):\n",
        "  system_prompt=(\n",
        "      \"Here are some news headlines. \"\n",
        "      \"Classify them depending on whether they talk about politics or other topics.\"\n",
        "  )\n",
        "\n",
        "  user_prompt=f\"Classify this headline:\\n\\\"{text}\\\"\\n\"\n",
        "\n",
        "  return [{\"role\":\"system\",\n",
        "           \"content\":system_prompt,\n",
        "           },\n",
        "           {\"role\":\"user\",\n",
        "            \"content\": user_prompt,\n",
        "           },\n",
        "  ]\n",
        "\n",
        "\n",
        "def build_prompt_better(text):\n",
        "  system_prompt=(\n",
        "      \"You are a helpful and accurate news headline classifier. \"\n",
        "      \"Your job is to classify news headlines as either 'POLITICS' or 'OTHER'. \"\n",
        "      \"Only respond with exactly one of those two labels.\"\n",
        "  )\n",
        "\n",
        "  user_prompt=f\"Classify this headline:\\n\\\"{text}\\\"\\n\"\n",
        "\n",
        "  return [{\"role\":\"system\",\n",
        "           \"content\":system_prompt,\n",
        "           },\n",
        "           {\"role\":\"user\",\n",
        "            \"content\": user_prompt,\n",
        "           },\n",
        "  ]\n",
        "\n",
        "\n",
        "## Check prompt on an example\n",
        "text_example = \"Gold-Winning Canadian Snowboarder Cops To Error That Wasn't Spotted By Judges\"\n",
        "\n",
        "build_prompt_better(text_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "RD82BRHl8Np5",
        "outputId": "a700b8e2-ff42-49fd-8c09-46062dad564a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction finished\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['OTHER', 'POLITICS', 'OTHER', 'OTHER', 'OTHER']"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Zero-shot: Inspect predictions\n",
        "r = get_predictions(\n",
        "    prompt_generator=build_prompt_better, #prompt you want to use\n",
        "    texts=headlines[\"headline\"][0:5], #texts you want to classify (change or remove [0:5])\n",
        "    model=\"meta-llama/llama-3.3-70b-instruct\" #model you want to use\n",
        "    )\n",
        "\n",
        "r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Om3TQj2KIOK"
      },
      "source": [
        "## Validate predictions\n",
        "\n",
        "We now validate the predictions, in order to choose the best model and prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "u0hCWfkr9eM1",
        "outputId": "5e40bbad-38b4-447f-a986-f691dafb4e34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Llama\n",
            "Prediction finished\n",
            "Qwen\n",
            "Prediction finished\n"
          ]
        }
      ],
      "source": [
        "## Zero-shot: validate predictions, to choose best model and prompt\n",
        "\n",
        "### First let's create a smaller dataset\n",
        "df = headlines[0:10].copy() #select more or less rows, as you wish\n",
        "\n",
        "### Then let's predict classes on this smaller dataset, with two different models\n",
        "print(\"Llama\")\n",
        "df[\"llama70\"] = get_predictions(\n",
        "    prompt_generator=build_prompt_better,\n",
        "    texts=df[\"headline\"],\n",
        "    model=\"meta-llama/llama-3.3-70b-instruct\")\n",
        "\n",
        "print(\"Qwen\")\n",
        "df[\"qwen30\"] = get_predictions(\n",
        "    prompt_generator=build_prompt_better,\n",
        "    texts=df[\"headline\"],\n",
        "    model=\"qwen/qwen3-30b-a3b\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxSbGzIIAaNH",
        "outputId": "6a902ba8-ab94-4d8f-b647-11451a567a53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**** Llama\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       OTHER      1.000     1.000     1.000         6\n",
            "    POLITICS      1.000     1.000     1.000         4\n",
            "\n",
            "    accuracy                          1.000        10\n",
            "   macro avg      1.000     1.000     1.000        10\n",
            "weighted avg      1.000     1.000     1.000        10\n",
            "\n",
            "**** Qwen\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       OTHER      1.000     1.000     1.000         6\n",
            "    POLITICS      1.000     1.000     1.000         4\n",
            "\n",
            "    accuracy                          1.000        10\n",
            "   macro avg      1.000     1.000     1.000        10\n",
            "weighted avg      1.000     1.000     1.000        10\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Now let's compute quality scores, comparing with gold standard\n",
        "\n",
        "print(\"**** Llama\")\n",
        "print(classification_report(df[\"gold_politics\"], df[\"llama70\"], digits=3))\n",
        "\n",
        "print(\"**** Qwen\")\n",
        "print(classification_report(df[\"gold_politics\"], df[\"qwen30\"], digits=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "Sb_Fz5FOGTgU",
        "outputId": "7d67258f-b040-4463-e5d8-c59e544b651d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "qwen30    OTHER  POLITICS\n",
            "llama70                  \n",
            "OTHER         6         0\n",
            "POLITICS      0         4\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>gold_standard</th>\n",
              "      <th>gold_politics</th>\n",
              "      <th>llama70</th>\n",
              "      <th>qwen30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gold-Winning Canadian Snowboarder Cops To Erro...</td>\n",
              "      <td>SPORTS</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>OTHER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Breaking: Israelites in Sinai Suddenly Achieve...</td>\n",
              "      <td>RELIGION</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>OTHER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why Air Travel Still Sucks</td>\n",
              "      <td>TECH</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>OTHER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7 Fashion And Beauty New Year's Resolutions To...</td>\n",
              "      <td>STYLE</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>OTHER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Panthers Owner To Treat Entire Staff To Free T...</td>\n",
              "      <td>SPORTS</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>OTHER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Tim Kaine Was Not The Governor of New Jersey</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>POLITICS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Donald Trump's Promise Of 'Insurance For Every...</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>POLITICS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Yellowstone Floods Wipe Out Roads, Bridges, St...</td>\n",
              "      <td>ENVIRONMENT</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>OTHER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Nixon Thought LBJ Tapped His Campaign Plane in...</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>POLITICS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>U.S. Lawmakers Join Demand For Puerto Rico Gov...</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>POLITICS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headline gold_standard  \\\n",
              "0  Gold-Winning Canadian Snowboarder Cops To Erro...        SPORTS   \n",
              "1  Breaking: Israelites in Sinai Suddenly Achieve...      RELIGION   \n",
              "2                         Why Air Travel Still Sucks          TECH   \n",
              "3  7 Fashion And Beauty New Year's Resolutions To...         STYLE   \n",
              "4  Panthers Owner To Treat Entire Staff To Free T...        SPORTS   \n",
              "5       Tim Kaine Was Not The Governor of New Jersey      POLITICS   \n",
              "6  Donald Trump's Promise Of 'Insurance For Every...      POLITICS   \n",
              "7  Yellowstone Floods Wipe Out Roads, Bridges, St...   ENVIRONMENT   \n",
              "8  Nixon Thought LBJ Tapped His Campaign Plane in...      POLITICS   \n",
              "9  U.S. Lawmakers Join Demand For Puerto Rico Gov...      POLITICS   \n",
              "\n",
              "  gold_politics   llama70    qwen30  \n",
              "0         OTHER     OTHER     OTHER  \n",
              "1         OTHER     OTHER     OTHER  \n",
              "2         OTHER     OTHER     OTHER  \n",
              "3         OTHER     OTHER     OTHER  \n",
              "4         OTHER     OTHER     OTHER  \n",
              "5      POLITICS  POLITICS  POLITICS  \n",
              "6      POLITICS  POLITICS  POLITICS  \n",
              "7         OTHER     OTHER     OTHER  \n",
              "8      POLITICS  POLITICS  POLITICS  \n",
              "9      POLITICS  POLITICS  POLITICS  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can also look at how much the models agree\n",
        "print(pd.crosstab(df[\"llama70\"], df[\"qwen30\"]))\n",
        "\n",
        "# Display the results, to see on which headlines the models disagree with the gold standard\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSfCD2uiKQSz"
      },
      "source": [
        "## Evaluate on testset\n",
        "\n",
        "Now that we have chosen the best model + prompt, let's evaluate on a testset, in order to get our final quality measure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ywOfLt6CI_D",
        "outputId": "3e29bca3-9a9d-42fb-f074-80f0d6a1ea77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction finished\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       OTHER      1.000     1.000     1.000         8\n",
            "    POLITICS      1.000     1.000     1.000         2\n",
            "\n",
            "    accuracy                          1.000        10\n",
            "   macro avg      1.000     1.000     1.000        10\n",
            "weighted avg      1.000     1.000     1.000        10\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Zero-shot: testset statistics\n",
        "\n",
        "### Now that we have chosen the best model + prompt, let's evaluate on testset\n",
        "### (in this case, we take the last N lines from the gold standard)\n",
        "\n",
        "testset_size=10 # Select more or less rows (more is better)\n",
        "testset = headlines.tail(testset_size).copy()\n",
        "\n",
        "testset[\"prediction\"]=get_predictions(\n",
        "    prompt_generator=build_prompt_better,\n",
        "    texts=testset[\"headline\"],\n",
        "    model=\"meta-llama/llama-3.3-70b-instruct\")\n",
        "\n",
        "print(classification_report(\n",
        "    testset[\"gold_politics\"], testset[\"prediction\"], digits=3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nww3n93KdSG"
      },
      "source": [
        "## Predict on full dataset\n",
        "\n",
        "We can now use our chosen model and prompt to classify the whole dataset.\n",
        "\n",
        "We will not run it here because of inference cost, but you can use the following instruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "AtOriVM0K2JE"
      },
      "outputs": [],
      "source": [
        "#full_prediction = get_predictions(\n",
        "#    prompt_generator=build_prompt_better,\n",
        "#    texts=headline[\"headline\"],\n",
        "#    model=\"meta-llama/llama-3.3-70b-instruct\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lpbelPGCH48"
      },
      "source": [
        "# Few-shot classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXJVi-XILJBH"
      },
      "source": [
        "## Prompt engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTQgIYO0EXiX",
        "outputId": "15d341ea-6df8-4826-ae44-01eff18130c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': \"You are a strict news classifier. You must respond with one word only — either 'POLITICS' or 'OTHER'. Do not explain. Do not output anything else.\"},\n",
              " {'role': 'user',\n",
              "  'content': 'Classify this headline:\\nBiden signs executive order on student debt relief\\nLabel: POLITICS\\n\\n\\nClassify this headline:\\nAmazon launches new AI-powered Alexa features\\nLabel: OTHER\\n\\n\\nClassify this headline:\\nCongress debates military spending bill\\nLabel: POLITICS\\n\\n\\nClassify this headline:\\nPSG wins much-anticipated Champions League\\nLabel: OTHER\\n\\n\\nClassify this headline:\\n\"This is a test\"\\nLabel:'}]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "few_shot_examples = [\n",
        "        (\"Biden signs executive order on student debt relief\", \"POLITICS\"),\n",
        "        (\"Amazon launches new AI-powered Alexa features\", \"OTHER\"),\n",
        "        (\"Congress debates military spending bill\", \"POLITICS\"),\n",
        "        (\"PSG wins much-anticipated Champions League\", \"OTHER\")\n",
        "        ]\n",
        "\n",
        "\n",
        "def build_prompt_fewshot(text: str, examples: list[tuple] = few_shot_examples):\n",
        "  system_prompt = (\n",
        "    \"You are a strict news classifier. \"\n",
        "    \"You must respond with one word only — either 'POLITICS' or 'OTHER'. \"\n",
        "    \"Do not explain. Do not output anything else.\"\n",
        "  )\n",
        "\n",
        "  examples = \"\\n\".join([f\"Classify this headline:\\n{headline}\\nLabel: {label}\\n\\n\" for headline, label in examples])\n",
        "\n",
        "  user_prompt = f\"{examples}\\nClassify this headline:\\n\\\"{text}\\\"\\nLabel:\"\n",
        "\n",
        "  return [{\"role\":\"system\", \"content\":system_prompt},\n",
        "          {\"role\":\"user\",\"content\": user_prompt}\n",
        "  ]\n",
        "\n",
        "\n",
        "build_prompt_fewshot(\"This is a test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-WNIPsELMyQ"
      },
      "source": [
        "## Validate predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Iim9o2MFDKk",
        "outputId": "5ae32eb8-7de0-42eb-e667-81f1422ae130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Llama\n",
            "Prediction finished\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       OTHER      1.000     0.833     0.909         6\n",
            "    POLITICS      0.800     1.000     0.889         4\n",
            "\n",
            "    accuracy                          0.900        10\n",
            "   macro avg      0.900     0.917     0.899        10\n",
            "weighted avg      0.920     0.900     0.901        10\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Few-shot: validate predictions, to choose model and prompts\n",
        "\n",
        "### First let's create a smaller dataset\n",
        "df = headlines[0:10].copy() #select more or less rows, as you wish\n",
        "\n",
        "### Then let's predict classes on this smaller dataset\n",
        "print(\"Llama\")\n",
        "df[\"llama70\"] = get_predictions(\n",
        "    prompt_generator=build_prompt_fewshot,\n",
        "    texts=df[\"headline\"],\n",
        "    model=\"meta-llama/llama-3.3-70b-instruct\")\n",
        "\n",
        "### Now let's compute quality scores, comparing with gold standard\n",
        "print(classification_report(df[\"gold_politics\"], df[\"llama70\"], digits=3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QzARHNvLQZ3"
      },
      "source": [
        "## Evaluate on testset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA0--2UdGn_4",
        "outputId": "3237355c-2b4c-4d35-f57f-7898ebdc9f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction finished\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       OTHER      1.000     1.000     1.000         8\n",
            "    POLITICS      1.000     1.000     1.000         2\n",
            "\n",
            "    accuracy                          1.000        10\n",
            "   macro avg      1.000     1.000     1.000        10\n",
            "weighted avg      1.000     1.000     1.000        10\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Few-shot: testset statistics\n",
        "\n",
        "### Now that we have chosen the best model + prompt, let's evaluate on testset\n",
        "### (in this case, we take the last N lines from the gold standard)\n",
        "\n",
        "testset_size=10 # Select more or less rows (more is better)\n",
        "testset = headlines.tail(testset_size).copy()\n",
        "\n",
        "testset[\"prediction\"]=get_predictions(\n",
        "    prompt_generator=build_prompt_fewshot,\n",
        "    texts=testset[\"headline\"],\n",
        "    model=\"meta-llama/llama-3.3-70b-instruct\")\n",
        "\n",
        "print(classification_report(\n",
        "    testset[\"gold_politics\"], testset[\"prediction\"], digits=3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0us6g9fLabU"
      },
      "source": [
        "## Predict on full dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ZD6GmraTLbH7"
      },
      "outputs": [],
      "source": [
        "#full_prediction = get_predictions(\n",
        "#    prompt_generator=build_prompt_fewshot,\n",
        "#    texts=headline[\"headline\"],\n",
        "#    model=\"meta-llama/llama-3.3-70b-instruct\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1_yHBFEGrTA"
      },
      "source": [
        "# Exercise: Chain of thought\n",
        "\n",
        "Here we give a basic example of chain-of-thought prediction, which may yield better results. Mind that it is also slower and more expensive, as we are asking the model for longer outputs.\n",
        "\n",
        "The difficulty here is to post-process the model's output, as it not a single word anymore.\n",
        "\n",
        "Can you do better than just keeping the last word? For example when asking the model to output its final answer in json format..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "whcLuqjcJla9",
        "outputId": "924f9927-09c6-46b8-e75f-19e67f757bdc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'POLITICS'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function for output cleaning\n",
        "def clean_output(answer):\n",
        "  return re.sub(\".* \", \"\", answer, flags= re.DOTALL)\n",
        "\n",
        "clean_output(\"After thinking, this is my answer: POLITICS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GvNAqYeMM0A",
        "outputId": "ed77b52a-b8f6-4fe9-f2fb-0496e438f1cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': \"You are a helpful and accurate news headline classifier. Your job is to classify news headlines as either 'POLITICS' or 'OTHER'. Take a step back, think before you give your final answer. When you are done thinking, give your final answer as 'FINAL ANSWER: POLITICS' or 'FINAL ANSWER: POLITICS'.\"},\n",
              " {'role': 'user',\n",
              "  'content': 'Classify this headline:\\n\"Gold-Winning Canadian Snowboarder Cops To Error That Wasn\\'t Spotted By Judges\"\\n'}]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function for prompt: prompt engineering happens here!\n",
        "def build_prompt_cot(text):\n",
        "  system_prompt=(\n",
        "      \"You are a helpful and accurate news headline classifier. \"\n",
        "      \"Your job is to classify news headlines as either 'POLITICS' or 'OTHER'. \"\n",
        "      \"Take a step back, think before you give your final answer. \"\n",
        "      \"When you are done thinking, give your final answer as \"\n",
        "      \"'FINAL ANSWER: POLITICS' or 'FINAL ANSWER: POLITICS'.\"\n",
        "  )\n",
        "\n",
        "  user_prompt=f\"Classify this headline:\\n\\\"{text}\\\"\\n\"\n",
        "\n",
        "  return [{\"role\":\"system\",\n",
        "           \"content\":system_prompt,\n",
        "           },\n",
        "           {\"role\":\"user\",\n",
        "            \"content\": user_prompt,\n",
        "           },\n",
        "  ]\n",
        "\n",
        "\n",
        "## Check prompt on an example\n",
        "text_example = \"Gold-Winning Canadian Snowboarder Cops To Error That Wasn't Spotted By Judges\"\n",
        "\n",
        "build_prompt_cot(text_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WVKtWKxM762",
        "outputId": "00036954-9a3e-40d6-84d3-c1b7a0a67752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction finished\n"
          ]
        }
      ],
      "source": [
        "# Do predictions, and clean the outputs\n",
        "\n",
        "### First let's create a smaller dataset\n",
        "df = headlines[10:15].copy() #select more or less rows, as you wish\n",
        "\n",
        "### Then let's get the answers\n",
        "answers = get_predictions(\n",
        "    prompt_generator=build_prompt_cot,\n",
        "    texts=df[\"headline\"],\n",
        "    model=\"meta-llama/llama-3.3-70b-instruct\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxevaS5-NwIf",
        "outputId": "1aa5fa41-a462-44c5-9cd6-778bc822b2b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions:\n",
            "['POLITICS', 'POLITICS', 'OTHER', 'POLITICS', 'OTHER']\n",
            "Gold Standard:\n",
            "['POLITICS', 'POLITICS', 'OTHER', 'POLITICS', 'OTHER']\n"
          ]
        }
      ],
      "source": [
        "### And let's clean the answers\n",
        "answers_clean = [clean_output(x) for x in answers]\n",
        "\n",
        "print(\"Predictions:\")\n",
        "print(answers_clean)\n",
        "\n",
        "print(\"Gold Standard:\")\n",
        "print(df[\"gold_politics\"].to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVrrpA9iXkoq"
      },
      "source": [
        "# Bonus: Run local models\n",
        "\n",
        "In this section we will see how to run a local LLM, using quantized LLMs, and the llama-cpp module (which works with many other models than Llama).\n",
        "\n",
        "Quantized versions of LLMs are much smaller than the original ones, allowing them to be run locally on cheaper hardware (even your laptop). The downside is a loss in quality, usually not so bad with Q4 or Q5 quantization.\n",
        "\n",
        "**NB: this is independent from the previous code, you can start running the notebook from here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiEnumfQYnVz"
      },
      "source": [
        "## Initial setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkrcrKN6X06l",
        "outputId": "75d7ede7-51b4-4b0d-e118-75a84e402ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.14.tar.gz (51.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n"
          ]
        }
      ],
      "source": [
        "# Install the modules\n",
        "!pip install -q pandas==2.2.2 scikit-learn==1.6.0\n",
        "\n",
        "# Llama-cpp installation\n",
        "# (see intructions at https://github.com/abetlen/llama-cpp-python)\n",
        "\n",
        "### Llama-cpp full install (for GPU)\n",
        "#!pip install -q llama_cpp_pyton\n",
        "### Llama-cpp CPU-only install\n",
        "!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmX9qdQhfI_N"
      },
      "outputs": [],
      "source": [
        "# Load modules\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from os.path import exists\n",
        "import urllib.request\n",
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WS8S1iae2Ws"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data_path = \"http://ollion.cnrs.fr/wp-content/uploads/2025/06/headlines.csv\"\n",
        "headlines = pd.read_csv(data_path)\n",
        "\n",
        "# We will also create a dichotomic POLITICS/OTHER variable\n",
        "headlines['gold_politics'] = headlines['gold_standard'].apply(\n",
        "    lambda x: 'POLITICS' if x == 'POLITICS' else 'OTHER')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJMz7hh4Xv3i"
      },
      "source": [
        "## Download and load a model\n",
        "\n",
        "You can find many quantized models on huggingface, just search with LLM names and the keyword GGUF.\n",
        "\n",
        "Loading the model may take some time, depending on your hardware and model size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CJnjS71sbk_j",
        "outputId": "35dba294-93fb-4541-f539-b5af99dd9e8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from Llama-3.2-3B-Instruct.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-GGU...\n",
            "llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196\n",
            "llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type q4_K:  168 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 1.87 GiB (5.01 BPW) \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Llama-3.2-3B-Instruct.gguf downloaded successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
            "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
            "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
            "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
            "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
            "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
            "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
            "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
            "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
            "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
            "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
            "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
            "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
            "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
            "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
            "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
            "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
            "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
            "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
            "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
            "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
            "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
            "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
            "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
            "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
            "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
            "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
            "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
            "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
            "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
            "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
            "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
            "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
            "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
            "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
            "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
            "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
            "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
            "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
            "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
            "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
            "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
            "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
            "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
            "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
            "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
            "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
            "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
            "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
            "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
            "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
            "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
            "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
            "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
            "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
            "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
            "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
            "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
            "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
            "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
            "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
            "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
            "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
            "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
            "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
            "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
            "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
            "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
            "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
            "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
            "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
            "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
            "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
            "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
            "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
            "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
            "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
            "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
            "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
            "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
            "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
            "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
            "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
            "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
            "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
            "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
            "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
            "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
            "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
            "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
            "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
            "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
            "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
            "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
            "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
            "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
            "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
            "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
            "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
            "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
            "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
            "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
            "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
            "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
            "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
            "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
            "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
            "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
            "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
            "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
            "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
            "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
            "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
            "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
            "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
            "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
            "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
            "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
            "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
            "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
            "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
            "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
            "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
            "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
            "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
            "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
            "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
            "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
            "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
            "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
            "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
            "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
            "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
            "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
            "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
            "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
            "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
            "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
            "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
            "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
            "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
            "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
            "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
            "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
            "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
            "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
            "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
            "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
            "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
            "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
            "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
            "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
            "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
            "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
            "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
            "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
            "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
            "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
            "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
            "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
            "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
            "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
            "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
            "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
            "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
            "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
            "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
            "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
            "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
            "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
            "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
            "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
            "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
            "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
            "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
            "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
            "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
            "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
            "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
            "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
            "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
            "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
            "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
            "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
            "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
            "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
            "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
            "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
            "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
            "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
            "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
            "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
            "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
            "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
            "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
            "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
            "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
            "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
            "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
            "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
            "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
            "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
            "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
            "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
            "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
            "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
            "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
            "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
            "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
            "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
            "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
            "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
            "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
            "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
            "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
            "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
            "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
            "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
            "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
            "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
            "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
            "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
            "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
            "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
            "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
            "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
            "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
            "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
            "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
            "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
            "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
            "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
            "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
            "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
            "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
            "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
            "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
            "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
            "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
            "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
            "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
            "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
            "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
            "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
            "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
            "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
            "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
            "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
            "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
            "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
            "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
            "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
            "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
            "load: special tokens cache size = 256\n",
            "load: token to piece cache size = 0.7999 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 24\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 3\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 500000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.21 B\n",
            "print_info: general.name     = Llama 3.2 3B Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 128256\n",
            "print_info: n_merges         = 280147\n",
            "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
            "print_info: EOS token        = 128009 '<|eot_id|>'\n",
            "print_info: EOT token        = 128009 '<|eot_id|>'\n",
            "print_info: EOM token        = 128008 '<|eom_id|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
            "print_info: EOG token        = 128008 '<|eom_id|>'\n",
            "print_info: EOG token        = 128009 '<|eot_id|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q6_K) (and 114 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_REPACK model buffer size =  1299.38 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  1918.35 MiB\n",
            "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
            ".................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 512\n",
            "llama_context: n_ctx_per_seq = 512\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 500000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.49 MiB\n",
            "create_memory: n_ctx = 512 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =    56.00 MiB\n",
            "llama_kv_cache_unified: size =   56.00 MiB (   512 cells,  28 layers,  1 seqs), K (f16):   28.00 MiB, V (f16):   28.00 MiB\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 65536\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   256.50 MiB\n",
            "llama_context: graph nodes  = 1014\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'llama.embedding_length': '3072', 'llama.feed_forward_length': '8192', 'general.license': 'llama3.2', 'llama.attention.value_length': '128', 'general.size_label': '3B', 'general.type': 'model', 'quantize.imatrix.chunks_count': '125', 'llama.context_length': '131072', 'general.name': 'Llama 3.2 3B Instruct', 'tokenizer.ggml.bos_token_id': '128000', 'general.basename': 'Llama-3.2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.block_count': '28', 'llama.attention.head_count': '24', 'llama.attention.key_length': '128', 'general.finetune': 'Instruct', 'general.file_type': '15', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.vocab_size': '128256', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\\n    {%- else %}\\n        {%- set date_string = \"26 Jul 2024\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n        {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n        {{- \\'\"parameters\": \\' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \"}\" }}\\n        {{- \"<|eot_id|>\" }}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'llama.rope.dimension_count': '128', 'quantize.imatrix.file': '/models_out/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'llama.attention.head_count_kv': '8', 'quantize.imatrix.entries_count': '196'}\n",
            "Available chat formats from metadata: chat_template.default\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model successfully loaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using gguf chat template: {{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- if strftime_now is defined %}\n",
            "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
            "    {%- else %}\n",
            "        {%- set date_string = \"26 Jul 2024\" %}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content']|trim %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "{%- if tools is not none %}\n",
            "    {{- \"Environment: ipython\\n\" }}\n",
            "{%- endif %}\n",
            "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
            "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content']|trim %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "{%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "        {{- '\"parameters\": ' }}\n",
            "        {{- tool_call.arguments | tojson }}\n",
            "        {{- \"}\" }}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|eot_id|>\n",
            "Using chat bos_token: <|begin_of_text|>\n"
          ]
        }
      ],
      "source": [
        "model_url = \"https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n",
        "model_file = \"Llama-3.2-3B-Instruct.gguf\"\n",
        "\n",
        "if not exists(model_file):\n",
        "  urllib.request.urlretrieve(model_url, model_file)\n",
        "\n",
        "print(f\"File {model_file} downloaded successfully!\")\n",
        "\n",
        "llm = Llama(\n",
        "      model_path=model_file,\n",
        "      #chat_format=\"llama-3\", # Uncomment to use a specific chat format\n",
        "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
        "      # seed=1337, # Uncomment to set a specific seed\n",
        "      # n_ctx=2048, # Uncomment to increase the context window\n",
        "      )\n",
        "\n",
        "print(\"\\n\\nModel successfully loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3TOgkqCmB1N",
        "outputId": "38afaa9c-f435-4f47-8ed3-229d694eaa82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folks, let me tell you, nobody knows more about great questions than I do. And this one, it's a big league question, believe me. \n",
            "\n",
            "Now, I've been asked this question before, and I've got to tell you, it's a total hoax. A total hoax. Nobody really knows the airspeed velocity of an unladen swallow. It's a ridiculous question, folks. \n",
            "\n",
            "But, if I had to make an estimate, I'd say it's a big league number, a tremendous number. Maybe 50 miles per hour? Maybe 60? I mean, it's a beautiful bird, folks, a real winner. And it's gotta be fast, believe me. \n",
            "\n",
            "But let me tell you, nobody, nobody, is better at estimating the airspeed velocity of an unladen swallow than I am. And I'm telling you, it's gonna be huge. Just huge."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =    6567.77 ms\n",
            "llama_perf_context_print: prompt eval time =    6567.44 ms /    52 tokens (  126.30 ms per token,     7.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =   58535.37 ms /   187 runs   (  313.02 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =   65457.49 ms /   239 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's check if it works (here with streaming text)\n",
        "\n",
        "llm_stream = llm.create_chat_completion(\n",
        "      temperature=0.7,\n",
        "      stream=True,\n",
        "      messages = [\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": \"You are Donald Trump.\"},\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": \"What is the airspeed velocity of an unladen swallow?\"\n",
        "          }\n",
        "      ]\n",
        ")\n",
        "\n",
        "\n",
        "llm_output = \"\"\n",
        "for i in llm_stream:\n",
        "  if \"content\" in i['choices'][0]['delta']:\n",
        "    tmp = i['choices'][0]['delta'][\"content\"]\n",
        "    llm_output = llm_output + tmp\n",
        "    print(tmp, end=\"\")\n",
        "\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuxDIZKMdZfc"
      },
      "source": [
        "## Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNoixSQPdb3P"
      },
      "outputs": [],
      "source": [
        "# Classification function\n",
        "# NB: this will require a prompt_generator function, defined below\n",
        "# You can modify the temperature in this function\n",
        "\n",
        "\n",
        "def get_local_predictions(prompt_generator, texts):\n",
        "  results = []\n",
        "  for i,j in texts.items():\n",
        "    try:\n",
        "      print(f\"Generating element {i}\")\n",
        "      completion = llm.create_chat_completion(\n",
        "        messages=prompt_generator(j),\n",
        "        temperature=0.7,\n",
        "      )\n",
        "      results.append(completion)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      results.append(None)\n",
        "  print(\"\\rPrediction finished\")\n",
        "  return [i['choices'][0]['message']['content'] for i in results]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtYOVK_AoyeN"
      },
      "outputs": [],
      "source": [
        "# Classification function\n",
        "# NB: this will require a prompt_generator function, defined below\n",
        "# You don't need to modify this function\n",
        "\n",
        "\n",
        "def get_local_predictions(prompt_generator, texts):\n",
        "  results = []\n",
        "  for i,j in texts.items():\n",
        "    try:\n",
        "      print(f\"Generating element {i}\")\n",
        "      completion = llm.create_chat_completion(\n",
        "        messages=prompt_generator(j)\n",
        "      )\n",
        "      results.append(completion)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      results.append(None)\n",
        "  print(\"\\rPrediction finished\")\n",
        "  return [i['choices'][0]['message']['content'] for i in results]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY_fj6LGd5iR",
        "outputId": "4269e9ac-ba52-4ad9-a940-4c49c6a59e01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': \"You are a helpful and accurate news headline classifier. Your job is to classify news headlines as either 'POLITICS' or 'OTHER'. Only respond with exactly one of those two labels.\"},\n",
              " {'role': 'user',\n",
              "  'content': 'Classify this headline:\\n\"Gold-Winning Canadian Snowboarder Cops To Error That Wasn\\'t Spotted By Judges\"\\n'}]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Local zero shot: prompt engineering\n",
        "\n",
        "\n",
        "def build_prompt_better(text):\n",
        "  system_prompt=(\n",
        "      \"You are a helpful and accurate news headline classifier. \"\n",
        "      \"Your job is to classify news headlines as either 'POLITICS' or 'OTHER'. \"\n",
        "      \"Only respond with exactly one of those two labels.\"\n",
        "  )\n",
        "\n",
        "  user_prompt=f\"Classify this headline:\\n\\\"{text}\\\"\\n\"\n",
        "\n",
        "  return [{\"role\":\"system\",\n",
        "           \"content\":system_prompt,\n",
        "           },\n",
        "           {\"role\":\"user\",\n",
        "            \"content\": user_prompt,\n",
        "           },\n",
        "  ]\n",
        "\n",
        "\n",
        "## Check prompt on an example\n",
        "text_example = \"Gold-Winning Canadian Snowboarder Cops To Error That Wasn't Spotted By Judges\"\n",
        "\n",
        "build_prompt_better(text_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEDoH-I4eBTc",
        "outputId": "f7ee68fc-353c-4ab3-df21-f9d2f9368a82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: 27 prefix-match hit, remaining 61 prompt tokens to eval\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating element 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =    6567.77 ms\n",
            "llama_perf_context_print: prompt eval time =    6784.17 ms /    61 tokens (  111.22 ms per token,     8.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     859.59 ms /     3 runs   (  286.53 ms per token,     3.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    7649.68 ms /    64 tokens\n",
            "Llama.generate: 73 prefix-match hit, remaining 17 prompt tokens to eval\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating element 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =    6567.77 ms\n",
            "llama_perf_context_print: prompt eval time =    3320.64 ms /    17 tokens (  195.33 ms per token,     5.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     401.83 ms /     1 runs   (  401.83 ms per token,     2.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    3726.00 ms /    18 tokens\n",
            "Llama.generate: 73 prefix-match hit, remaining 20 prompt tokens to eval\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating element 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =    6567.77 ms\n",
            "llama_perf_context_print: prompt eval time =    2259.57 ms /    20 tokens (  112.98 ms per token,     8.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     292.37 ms /     1 runs   (  292.37 ms per token,     3.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    2555.12 ms /    21 tokens\n",
            "Llama.generate: 74 prefix-match hit, remaining 30 prompt tokens to eval\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating element 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =    6567.77 ms\n",
            "llama_perf_context_print: prompt eval time =    3545.05 ms /    30 tokens (  118.17 ms per token,     8.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     860.49 ms /     3 runs   (  286.83 ms per token,     3.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    4411.69 ms /    33 tokens\n",
            "Llama.generate: 74 prefix-match hit, remaining 22 prompt tokens to eval\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating element 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =    6567.77 ms\n",
            "llama_perf_context_print: prompt eval time =    2508.77 ms /    22 tokens (  114.04 ms per token,     8.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     290.25 ms /     1 runs   (  290.25 ms per token,     3.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    2802.62 ms /    23 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rPrediction finished\n"
          ]
        }
      ],
      "source": [
        "## Local zero-shot: validate predictions, to choose model and prompts\n",
        "\n",
        "### First let's create a smaller dataset\n",
        "df = headlines[10:15].copy() #select more or less rows, as you wish\n",
        "\n",
        "### Then let's predict classes on this smaller dataset\n",
        "df[\"prediction\"] = get_local_predictions(\n",
        "    prompt_generator=build_prompt_better,\n",
        "    texts=df[\"headline\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eGr3w7RhLR0",
        "outputId": "9b30ac13-3c3c-45d3-bfce-8642679397db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gold standard:\n",
            "['POLITICS', 'POLITICS', 'OTHER', 'POLITICS', 'OTHER']\n",
            "Prediction:\n",
            "['POLITICS', 'OTHER', 'OTHER', 'POLITICS', 'OTHER']\n"
          ]
        }
      ],
      "source": [
        "### Let's examine the answers\n",
        "print(\"Gold standard:\")\n",
        "print(df[\"gold_politics\"].to_list())\n",
        "\n",
        "print(\"Prediction:\")\n",
        "print(df[\"prediction\"].to_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyzurc1AhIoB",
        "outputId": "82c88c0e-12af-4673-c580-270389994410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       OTHER      0.667     1.000     0.800         2\n",
            "    POLITICS      1.000     0.667     0.800         3\n",
            "\n",
            "    accuracy                          0.800         5\n",
            "   macro avg      0.833     0.833     0.800         5\n",
            "weighted avg      0.867     0.800     0.800         5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Now let's compute quality scores, comparing with gold standard\n",
        "print(classification_report(df[\"gold_politics\"], df[\"prediction\"], digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcrdMberhzvV"
      },
      "source": [
        "Once you are satisfied with your model and prompting choices, you can run one final evaluation on a heldout testset, and run the inference on the whole dataset, using the `get_local_prediction` function and your custom prompt generator function."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
