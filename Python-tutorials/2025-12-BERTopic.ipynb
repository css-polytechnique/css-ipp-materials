{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements: \n",
    "```bash\n",
    "bertopic==0.17.3\n",
    "datasets==4.3.0\n",
    "hdbscan==0.8.40\n",
    "numpy==2.3.5\n",
    "pandas==2.3.3\n",
    "plotly==6.3.1\n",
    "scikit_learn==1.8.0\n",
    "stopwordsiso==0.6.1\n",
    "transformers==4.52.4\n",
    "umap_learn==0.5.7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and seed setting \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoConfig \n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from stopwordsiso import stopwords\n",
    "from datasets import load_from_disk, Dataset\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import plotly.graph_objects as go \n",
    "\n",
    "RANDOM_SEED = 2306406\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "default_umap_parameters = {\n",
    "    \"n_neighbors\":15, \n",
    "    \"n_components\":5,\n",
    "    \"min_dist\":0.0, \n",
    "    \"metric\":'cosine'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Read the introduction and the theory behind BERTopic on our [website](https://css-polytechnique.github.io/css-ipp-materials/pages/bertopic-tutorial.html)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic pipeline: the essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, we will use the dataset listing all dissertations defended in France since 1985. The original dataset can be downloaded on [data.gouv.fr](https://www.data.gouv.fr/datasets/theses-soutenues-en-france-depuis-1985/). To avoid excessive pre-processing, we curated[^17] the dataset and uploaded it (with the code) on [Zenodo](https://doi.org/10.5281/zenodo.17936090).\n",
    "\n",
    "It is crucial to stress that the preprocessing step is **the most important step**. Although we can tune the topic model towards meaningful clusters and representations, your corpus is your input and no model will generate good results out of poor inputs. We list a number of questions you need to consider and justify for your topic model to be relevant:\n",
    "\n",
    "**Is my corpus homogeneous enough?**\n",
    "\n",
    "It could be tempting to shove millions of different documents from different sources into a topic model and see what comes out. However, to make sure that the groups will represent topics, one must be sure that your documents are similar in formality, tone, length, density of information etc... If your corpus is too heterogeneous, the topic model can highlight these differences and you will lose sight of meaningful latent topics[^6].\n",
    "\n",
    "In our case, as we analyse dissertation abstracts which are quite standardised, the corpus should be homogenous enough for the topic model to pick up topics and not other semantic dimensions. It is worth noting that using the abstracts as a proxy to analyse a corpus of papers is common practice (Ma et al., 2025; Ollion et al., 2025)\n",
    "\n",
    "**Are my documents in the right language?**\n",
    "\n",
    "Most of the time, language models are trained in a single language. Some models are called multi-lingual and accept texts in more than one language. However, in our experience, working with documents in different languages generates poor topics as the language shift holds for the most salient difference and each language is clustered by itself. We recommend translating your documents in a single language beforehand.\n",
    "\n",
    "In our case, the data curation led us to extract dissertations where both the English and the French abstracts were provided and we will work with abstracts in each language separately.\n",
    "\n",
    "**How long are my documents?**\n",
    "\n",
    "One needs to precisely define their task before diving into topic modelling. What are you trying to analyse? Will this information be available at the sentence level? paragraph level? the document level?\n",
    "\n",
    "In our case, the topic of the dissertations will be described throughout the abstract, hence the abstract must be taken as a whole and not subdivided at the sentence level.\n",
    "\n",
    "Also, as introduced before, each embedding model has a context window, meaning that long documents will be truncated. One must make sure that the length of the documents in their corpus is smaller than the model's context window. If the context window is too small consider changing embedding model. Careful though, larger context window means longer computation time and greater computation resources required to run the model.\n",
    "\n",
    "We will confirm the length of our documents before using the embedding model.\n",
    "\n",
    "[^17]: Curation can be found [here](https://css-polytechnique.github.io/css-ipp-materials/pages/techy-notes.html#instances-file-size-and-content)\n",
    "\n",
    "[^6]: Disclaimer: You may want to highlight these dimensions to identify hate speech, for instance. Homogeneity has to be relevant to your use case and questioning your corpus is a part of the topic modelling pipeline that should not remain overlooked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open your data \n",
    "\n",
    "Let's load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CI</th>\n",
       "      <th>year</th>\n",
       "      <th>oai_set_specs</th>\n",
       "      <th>titres.en</th>\n",
       "      <th>resumes.en</th>\n",
       "      <th>lang_res.en</th>\n",
       "      <th>topics.en</th>\n",
       "      <th>titres.fr</th>\n",
       "      <th>resumes.fr</th>\n",
       "      <th>lang_res.fr</th>\n",
       "      <th>topics.fr</th>\n",
       "      <th>swapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CI-0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>ddc:004</td>\n",
       "      <td>ViSaGe project : VisageFS, a filesystem with a...</td>\n",
       "      <td>Nowdays, the grid computing enables solutions ...</td>\n",
       "      <td>EN</td>\n",
       "      <td>POSIX (norme)</td>\n",
       "      <td>Projet ViSaGe : VisageFS, systèmes de fichiers...</td>\n",
       "      <td>Les grilles informatiques permettent d'envisag...</td>\n",
       "      <td>FR</td>\n",
       "      <td>Entrepôts de données||Langages de programmatio...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CI-1</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>ddc:570</td>\n",
       "      <td>Neural basis of glaucoma : a new approach comb...</td>\n",
       "      <td>Decreased visual motion sensitivity in early s...</td>\n",
       "      <td>EN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bases neuronales du glaucome : une approche co...</td>\n",
       "      <td>La diminution précoce de la sensibilité au mou...</td>\n",
       "      <td>FR</td>\n",
       "      <td>Poursuite oculaire||Glaucome à angle ouvert</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CI-2</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>ddc:150</td>\n",
       "      <td>Richard Wagner and the Redemption's opera : co...</td>\n",
       "      <td>Richard Wagner's poetic and musical writing in...</td>\n",
       "      <td>EN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Richard Wagner et l’Opéra de la Rédemption : c...</td>\n",
       "      <td>L’écriture poétique et musicale de Richard Wag...</td>\n",
       "      <td>FR</td>\n",
       "      <td>Musique -- 19e siècle -- Thèmes, motifs||Psych...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CI-3</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>ddc:530</td>\n",
       "      <td>Investigation of temperature measurement of ma...</td>\n",
       "      <td>This work investigates the temperature measure...</td>\n",
       "      <td>EN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Contribution à la mesure de température des ma...</td>\n",
       "      <td>Le cadre de ces travaux concerne la mesure de ...</td>\n",
       "      <td>FR</td>\n",
       "      <td>Thermométrie||Pyrométrie||Choc (mécanique)||Ma...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CI-4</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>ddc:796</td>\n",
       "      <td>Sociology of juvenile prison</td>\n",
       "      <td>Researches in social sciences that deal with t...</td>\n",
       "      <td>EN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Faire sa peine à l'établissement pénitentiaire...</td>\n",
       "      <td>Les recherches en sciences sociales s'intéress...</td>\n",
       "      <td>FR</td>\n",
       "      <td>Centres pour jeunes délinquants||Détention des...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CI    year oai_set_specs  \\\n",
       "0  CI-0  2010.0       ddc:004   \n",
       "1  CI-1  2012.0       ddc:570   \n",
       "2  CI-2  2010.0       ddc:150   \n",
       "3  CI-3  2010.0       ddc:530   \n",
       "4  CI-4  2012.0       ddc:796   \n",
       "\n",
       "                                           titres.en  \\\n",
       "0  ViSaGe project : VisageFS, a filesystem with a...   \n",
       "1  Neural basis of glaucoma : a new approach comb...   \n",
       "2  Richard Wagner and the Redemption's opera : co...   \n",
       "3  Investigation of temperature measurement of ma...   \n",
       "4                       Sociology of juvenile prison   \n",
       "\n",
       "                                          resumes.en lang_res.en  \\\n",
       "0  Nowdays, the grid computing enables solutions ...          EN   \n",
       "1  Decreased visual motion sensitivity in early s...          EN   \n",
       "2  Richard Wagner's poetic and musical writing in...          EN   \n",
       "3  This work investigates the temperature measure...          EN   \n",
       "4  Researches in social sciences that deal with t...          EN   \n",
       "\n",
       "       topics.en                                          titres.fr  \\\n",
       "0  POSIX (norme)  Projet ViSaGe : VisageFS, systèmes de fichiers...   \n",
       "1            NaN  Bases neuronales du glaucome : une approche co...   \n",
       "2            NaN  Richard Wagner et l’Opéra de la Rédemption : c...   \n",
       "3            NaN  Contribution à la mesure de température des ma...   \n",
       "4            NaN  Faire sa peine à l'établissement pénitentiaire...   \n",
       "\n",
       "                                          resumes.fr lang_res.fr  \\\n",
       "0  Les grilles informatiques permettent d'envisag...          FR   \n",
       "1  La diminution précoce de la sensibilité au mou...          FR   \n",
       "2  L’écriture poétique et musicale de Richard Wag...          FR   \n",
       "3  Le cadre de ces travaux concerne la mesure de ...          FR   \n",
       "4  Les recherches en sciences sociales s'intéress...          FR   \n",
       "\n",
       "                                           topics.fr swapped  \n",
       "0  Entrepôts de données||Langages de programmatio...     NaN  \n",
       "1        Poursuite oculaire||Glaucome à angle ouvert     NaN  \n",
       "2  Musique -- 19e siècle -- Thèmes, motifs||Psych...     NaN  \n",
       "3  Thermométrie||Pyrométrie||Choc (mécanique)||Ma...     NaN  \n",
       "4  Centres pour jeunes délinquants||Détention des...     NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"./data/theses-soutenues-curated.csv\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following columns :\n",
    "\n",
    "- `CI`: Custom index, values are `CI-XXXX`, with `XXXX` ranging from 0 to 164,378\n",
    "- `year`: the year of the defence, values are integers ranging from 2010 to 2022\n",
    "- `oai_set_specs`: the oai code, each code looks like `ddc:XXX`, for instance `ddc:300` refers to `Sciences sociales, sociologie, anthropologie`.\n",
    "- `resumes.en` and `resumes.fr`: the abstract of the PhD dissertation, respectively in English and French. We are sure that every row contains a valid abstract in the right language thanks to the data curation.\n",
    "- `titres.en` and `titres.fr`: the titles of the PhD dissertation, respectively in English and French. Only 5% of the rows do not have a valid title (French or English). The language of the titles has not been checked because it will only be used to check the qualitative validity of topic model.\n",
    "<!-- TODO: Make sure that we use the titres.en and topics.en somewhere -->\n",
    "- `topics.en` and `topics.fr`: the aggregated topics provided by the author. Only 5% of the rows do not have valid topics (French or English). The language of the topics has not been checked because they will only be used to check the qualitative validity of topic model.\n",
    "\n",
    "Let's take some time to check if our documents fit inside the context window.\n",
    "To retrieve the context window size, you can check the HuggingFace page of the model or load the configuration file that contains this information as such:\n",
    "\n",
    "```python\n",
    "from transformers import AutoConfig \n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code = True)\n",
    "print(f\"Context window size of the model {model_name}: {config.max_position_embeddings}\")\n",
    "```\n",
    "\n",
    "Let’s look at two models, sentence-transformers/all-MiniLM-L6-v2 (default embedding model in the BERTopic pipeline) and Alibaba-NLP/gte-multilingual-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context window size of the model sentence-transformers/all-MiniLM-L6-v2: 512\n",
      "Context window size of the model Alibaba-NLP/gte-multilingual-base: 8192\n"
     ]
    }
   ],
   "source": [
    "for model_name in ['sentence-transformers/all-MiniLM-L6-v2', 'Alibaba-NLP/gte-multilingual-base']:\n",
    "    config = AutoConfig.from_pretrained(model_name, trust_remote_code = True)\n",
    "    print(f\"Context window size of the model {model_name}: {config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let’s look at the length of our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resumes.en.len</th>\n",
       "      <th>resumes.fr.len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>164379.000000</td>\n",
       "      <td>164379.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1777.648082</td>\n",
       "      <td>1984.935119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>735.027732</td>\n",
       "      <td>802.720810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1324.000000</td>\n",
       "      <td>1508.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1617.000000</td>\n",
       "      <td>1702.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2080.000000</td>\n",
       "      <td>2362.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12010.000000</td>\n",
       "      <td>12207.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       resumes.en.len  resumes.fr.len\n",
       "count   164379.000000   164379.000000\n",
       "mean      1777.648082     1984.935119\n",
       "std        735.027732      802.720810\n",
       "min          1.000000        6.000000\n",
       "25%       1324.000000     1508.000000\n",
       "50%       1617.000000     1702.000000\n",
       "75%       2080.000000     2362.000000\n",
       "max      12010.000000    12207.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw[\"resumes.en.len\"] = df_raw[\"resumes.en\"].apply(len)\n",
    "df_raw[\"resumes.fr.len\"] = df_raw[\"resumes.fr\"].apply(len)\n",
    "df_raw.loc[:,[\"resumes.en.len\", \"resumes.fr.len\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these statistics, we see that we can rule out using `sentence-transformers/all-MiniLM-L6-v2` because it's context window is too narrow. By keeping abstracts between 1000 and 4000 characters (ie between 300 and 1300 tokens) we can retain most of the dataset (89%) while maintaining a reasonable computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_index = np.logical_and.reduce([\n",
    "\tdf_raw[\"resumes.fr.len\"] >= 1000,\n",
    "\tdf_raw[\"resumes.fr.len\"] <= 4000,\n",
    "\tdf_raw[\"resumes.en.len\"] >= 1000,\n",
    "\tdf_raw[\"resumes.en.len\"] <= 4000,\n",
    "])\n",
    "\n",
    "df = df_raw.loc[valid_index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if it is more interesting to process the complete dataset, it can be computationally expensive. To limit computation time at least for the exploratory steps, we are going to work on a sample of documents. To maintain some representativeness, we stratify this sampling by the year of the defence[^8].\n",
    "\n",
    "[^8]: More information on [stratification in Pandas](https://proclusacademy.com/blog/stratified_sampling_pandas/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratification_column = \"year\"\n",
    "samples_per_stratum = 500\n",
    "df_stratified = (\n",
    "\tdf\n",
    "\t.groupby(stratification_column, as_index = False)\n",
    "\t.apply(lambda x : x.sample(n = samples_per_stratum), include_groups=True)\n",
    "\t.reset_index()\n",
    "\t.drop([\"level_0\", \"level_1\"], axis = 1)\n",
    ")\n",
    "# Save the preprocessed dataset\n",
    "df_stratified.to_csv(\"./data/theses-soutenues-curated-stratified.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting stratified table contains 6500 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a BERTopic object, fit and transform {#sec-create-instance}\n",
    "\n",
    "To create a `topic_model` object we need to create a `BERTopic` object and define some parameters. For now, we will not change the default parameters of the clustering model (`hdbscan_model`) or the dimension reduction model (`umap_model`). We will however define the language of the corpus as well as the vectorizer model in order to remove all stopwords and retrieve meaningful topics. Then, one must use the `fit` method to fit the topic model to the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"english\" # or \"french\"\n",
    "language_short = language[:2] # \"en\" or \"fr\"\n",
    "embedding_model = \"Alibaba-NLP/gte-multilingual-base\"\n",
    "docs = df_stratified[f\"resumes.{language_short}\"]\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words = list(stopwords(language_short)))\n",
    "\n",
    "topic_model = BERTopic(\n",
    "\tlanguage = language,\n",
    "    embedding_model = embedding_model,\n",
    "\tvectorizer_model = vectorizer_model,\n",
    ")\n",
    "topic_model.fit(documents=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet of code takes a long time to run (>10 mins) as each element must be embedded first.\n",
    "\n",
    "To avoid unnecessary computation time, we have embedded 6500 elements with several models for the French and English abstracts that you can download from [Zenodo](https://doi.org/10.5281/zenodo.17936090). The code changes as such: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x3bd4d57f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language = \"english\" # or \"french\"\n",
    "language_short = language[:2] # \"en\" or \"fr\"\n",
    "embedding_model = \"answerdotai/ModernBERT-base\"\n",
    "# docs = df_stratified[f\"resumes.{language_short}\"]\n",
    "\n",
    "# Load the embeddings directly to avoid long computation time\n",
    "ds = load_from_disk(f\"./data/embeddings/gte-multilingual-base-{language_short}-SBERT\")\n",
    "docs = np.array(ds[f\"resumes.{language_short}\"]) # 6500 rows\n",
    "embeddings = np.array(ds[\"embedding\"])\t\t\t # Shape : 6500 x 768\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words = list(stopwords(language_short)))\n",
    "\n",
    "topic_model = BERTopic(\n",
    "\tlanguage = language,\n",
    "    # Remove Stopwords to retrieve better keywords\n",
    "\tvectorizer_model = vectorizer_model,\n",
    "    # Impose the random seed to produce deterministic results\n",
    "    umap_model= UMAP(**default_umap_parameters, random_state=RANDOM_SEED)\n",
    ")\n",
    "topic_model.fit(documents=docs, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to extract the topics, we need to call the `transform` method : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probabilities = topic_model.transform(documents=docs, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to explore the topics we can call the `get_topics_info` method that will return a table with the keywords, representative documents and the number of documents in each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>2715</td>\n",
       "      <td>-1_study_thesis_model_based</td>\n",
       "      <td>[study, thesis, model, based, analysis, data, ...</td>\n",
       "      <td>[The development of machine learning technique...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "      <td>0_literary_writing_art_contemporary</td>\n",
       "      <td>[literary, writing, art, contemporary, authors...</td>\n",
       "      <td>[A particular idea of literature was born duri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>1_mechanical_numerical_behavior_material</td>\n",
       "      <td>[mechanical, numerical, behavior, material, cr...</td>\n",
       "      <td>[In the field of construction, concrete is the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>2_cells_cancer_tumor_cell</td>\n",
       "      <td>[cells, cancer, tumor, cell, immune, expressio...</td>\n",
       "      <td>[The dendritic cells, principal antigen presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>155</td>\n",
       "      <td>3_flow_numerical_fluid_acoustic</td>\n",
       "      <td>[flow, numerical, fluid, acoustic, flame, flow...</td>\n",
       "      <td>[Bubbly flows occurring in nuclear power plant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>100_xe_adsorption_zeolite_diffusion</td>\n",
       "      <td>[xe, adsorption, zeolite, diffusion, cement, s...</td>\n",
       "      <td>[Storage of Xe in silicate minerals has been p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>101</td>\n",
       "      <td>10</td>\n",
       "      <td>101_influenza_vaccination_meningitis_virus</td>\n",
       "      <td>[influenza, vaccination, meningitis, virus, in...</td>\n",
       "      <td>[Molecular assays are frequently requested for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>102</td>\n",
       "      <td>10</td>\n",
       "      <td>102_ablation_atrial_intracranial_vein</td>\n",
       "      <td>[ablation, atrial, intracranial, vein, cardiac...</td>\n",
       "      <td>[Objectives. The aim of our studies was to eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>103</td>\n",
       "      <td>10</td>\n",
       "      <td>103_building_ventilation_thermal_heat</td>\n",
       "      <td>[building, ventilation, thermal, heat, cooling...</td>\n",
       "      <td>[The building sector has a major role to play ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>104</td>\n",
       "      <td>10</td>\n",
       "      <td>104_humins_tannins_biobased_foams</td>\n",
       "      <td>[humins, tannins, biobased, foams, composites,...</td>\n",
       "      <td>[An alternative to industrial phenol or resorc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                        Name  \\\n",
       "0       -1   2715                 -1_study_thesis_model_based   \n",
       "1        0    262         0_literary_writing_art_contemporary   \n",
       "2        1    172    1_mechanical_numerical_behavior_material   \n",
       "3        2    172                   2_cells_cancer_tumor_cell   \n",
       "4        3    155             3_flow_numerical_fluid_acoustic   \n",
       "..     ...    ...                                         ...   \n",
       "101    100     10         100_xe_adsorption_zeolite_diffusion   \n",
       "102    101     10  101_influenza_vaccination_meningitis_virus   \n",
       "103    102     10       102_ablation_atrial_intracranial_vein   \n",
       "104    103     10       103_building_ventilation_thermal_heat   \n",
       "105    104     10           104_humins_tannins_biobased_foams   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [study, thesis, model, based, analysis, data, ...   \n",
       "1    [literary, writing, art, contemporary, authors...   \n",
       "2    [mechanical, numerical, behavior, material, cr...   \n",
       "3    [cells, cancer, tumor, cell, immune, expressio...   \n",
       "4    [flow, numerical, fluid, acoustic, flame, flow...   \n",
       "..                                                 ...   \n",
       "101  [xe, adsorption, zeolite, diffusion, cement, s...   \n",
       "102  [influenza, vaccination, meningitis, virus, in...   \n",
       "103  [ablation, atrial, intracranial, vein, cardiac...   \n",
       "104  [building, ventilation, thermal, heat, cooling...   \n",
       "105  [humins, tannins, biobased, foams, composites,...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [The development of machine learning technique...  \n",
       "1    [A particular idea of literature was born duri...  \n",
       "2    [In the field of construction, concrete is the...  \n",
       "3    [The dendritic cells, principal antigen presen...  \n",
       "4    [Bubbly flows occurring in nuclear power plant...  \n",
       "..                                                 ...  \n",
       "101  [Storage of Xe in silicate minerals has been p...  \n",
       "102  [Molecular assays are frequently requested for...  \n",
       "103  [Objectives. The aim of our studies was to eva...  \n",
       "104  [The building sector has a major role to play ...  \n",
       "105  [An alternative to industrial phenol or resorc...  \n",
       "\n",
       "[106 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Representation\" column provides the keywords for each topic. We can see that the keywords retrieved for the noise cluster are very generic: \"study\", \"thesis\", \"model\", \"analysis\", \"approach\" and do not convey any meaningful information other than the fact that all documents are academic documents.\n",
    "\n",
    "For each topic, the model identifies apparently consistent keywords : \n",
    "\n",
    "- 'mechanical', 'numerical', 'model', 'finite', 'element': this cluster may be grouping dissertations related to numerical simulation in mechanics using the finite element method.\n",
    "- ‘cells’, ‘cancer’, ‘tumor’, ‘immune’, ‘patients’: this cluster may be grouping dissertations related to cancer and cures.\n",
    "- ‘building’, ‘ventilation’, ‘heat’, ‘cooling’: this cluster may be grouping dissertations related to the thermodynamics of buildings.\n",
    "\n",
    "This is not a proof that our model generated interesting results, for that we need to carry further investigation of each cluster. Still this is a first step towards assessing the quality of the topic model.\n",
    "\n",
    "In this table we can see that almost half of the documents are classified as noise (topic -1). This is a normal behaviour of the clustering algorithm as it focuses on dense areas first. This way, the topic model creates representations that focus fewer documents to retrieve very specific keywords. The other 104 topics contain between 10 and 200 documents each, which correspond to 0.1% to 3% of the corpus. \n",
    "\n",
    "It is worth noting that the more documents in a cluster, the lower the topic index is. \n",
    "\n",
    "It is possible to re-assign clusters to documents clustered as noise with the `reduce_outliers` method, based on the documentation and the code, we recommend using the *\"embedding\"* strategy[^22]:\n",
    "\n",
    "[^22]: See [techy-notes](https://css-polytechnique.github.io/css-ipp-materials/pages/techy-notes.html#reduce-outliers-strategies) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_no_outliers = topic_model.reduce_outliers(\n",
    "    documents = docs, \n",
    "    topics = topics, \n",
    "    probabilities = probabilities, \n",
    "    embeddings = embeddings,\n",
    "    strategy=\"embeddings\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "\n",
    "The topic model is not altered and keywords are not re-generated.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise your results\n",
    "\n",
    "Visualising your topics is central in topic modelling as this is the most convenient way to explore your documents and your topic model. We are going to cover some of the most basic and helpful visualisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D plot\n",
    "\n",
    "The first thing we want to visualise is the space of the embeddings reduced to 2 dimensions. This is a good start to visualise the size of your clusters and if close clusters have similar topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "\ttopic_model\n",
    "\t.visualize_documents(\n",
    "\t\tdocs = docs,\n",
    "\t\tembeddings = embeddings,\n",
    "\t\thide_annotations = True, # better readability\n",
    "\t\ttopics = [0,1,2,3],      # Select topics to highlight\n",
    "\t\theight = 600, # Adjust the height of the plot\n",
    "\t\twidth = 800 # Adjust the width of the plot \n",
    "\t)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_run cell to see the visualisation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this graph only displaying the 4 top topics, we can see that the cluster are large and dense. We can see that the \"1_mechanical_numerical_behavior\" and the \"3_flow_numerical_fluid\" are close, as expected whereas \"0_literary_writing_art\" and \"2_cells_cancer_tumor\" are further apart in separate directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise top words per topic\n",
    "\n",
    "The second helpful visualisation is to illustrate the top $n$ words that represent each topic and how representative they are of a given topic. It is a good way to analyse the consistency of each topic and get a sense of the documents inside a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(\n",
    "\tn_words = 10, # Select the number of words to display per topic\n",
    "\t# topics = [0,1,2,3,4], # Select specific topics to display\n",
    "\t# top_n_topics = 6, # Select the first n topics to display\n",
    "\theight = 300, # Adjust the height of the plot\n",
    "\t# width = 800 # Adjust the width of the plot \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_run cell to see the visualisation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the topic n°5, we can see keywords like \"urban\", \"land\", \"city\", \"local\", \"public\" and \"ecological\". These keywords make sense together and we can imagine dissertations discussing urban planning at different scales and under different constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical trees\n",
    "\n",
    "A good way to visualise how topics interact with each other is to plot a dendogram. This plot is read from left to right, the sooner branches merge, the closer related topics are. We use the `visualize_hierarchy` method. The graph is very tall but we can easily focus on one subset of the tree at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_run cell to see the visualisation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the very top of this graph, we can see the green branches related to laws and international law, very close to contract law. This is a sign that these three branches merge together and that we dont find legislation-related keywords anywhere else in the three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic pipeline: advanced practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate topics{#sec-aggregate-topics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, the topic model will generate many groups, with some groups that you'd want to merge together.\n",
    "\n",
    "To aggregate topics, the algorithm proposed is to use the topic embedding (the mean of the document's embedding inside a cluster), compute the cosine similarity[^26] matrix and use the agglomerative clustering algorithm described [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) to aggregate the topics. Once performed, all the documents are moved into a new group and keywords are re-generated.\n",
    "\n",
    "With more than 100 topics, it is difficult to have a general idea of the main groups in the corpus. Looking back to the Figure 6 we can identify seven large branches we could reduce our topic model to.\n",
    "\n",
    "[^26]: Cosine similarity is a metric bound between 0 and 1 and is a proxy for the semantic distance between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.reduce_topics(docs = docs, nr_topics=7 + 1) #Add one to account for the noise\n",
    "#Retrieve the updated topics and probabilities\n",
    "topics_reduced, probabilities_reduced = topic_model.topics_, topic_model.probabilities_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then check the new representations like in the section 3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>2715</td>\n",
       "      <td>-1_study_thesis_model_based</td>\n",
       "      <td>[study, thesis, model, based, analysis, data, ...</td>\n",
       "      <td>[Inspired by the limitations of traditional PI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1037</td>\n",
       "      <td>0_study_analysis_social_thesis</td>\n",
       "      <td>[study, analysis, social, thesis, french, poli...</td>\n",
       "      <td>[The impact study is an approach to support ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>795</td>\n",
       "      <td>1_cells_cell_expression_role</td>\n",
       "      <td>[cells, cell, expression, role, species, study...</td>\n",
       "      <td>[The dendritic cells, principal antigen presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>707</td>\n",
       "      <td>2_model_numerical_study_method</td>\n",
       "      <td>[model, numerical, study, method, experimental...</td>\n",
       "      <td>[Regardless the industry, additive manufacturi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>634</td>\n",
       "      <td>3_data_thesis_systems_model</td>\n",
       "      <td>[data, thesis, systems, model, propose, based,...</td>\n",
       "      <td>[Today, diseases and illnesses are becoming th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>464</td>\n",
       "      <td>4_properties_magnetic_surface_synthesis</td>\n",
       "      <td>[properties, magnetic, surface, synthesis, mat...</td>\n",
       "      <td>[Magnetic nanoparticles are now used in a wide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>135</td>\n",
       "      <td>5_law_legal_international_rights</td>\n",
       "      <td>[law, legal, international, rights, european, ...</td>\n",
       "      <td>[This thesis revisits the classical doctrine o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>6_detection_biosensor_biosensors_based</td>\n",
       "      <td>[detection, biosensor, biosensors, based, dna,...</td>\n",
       "      <td>[This work presents the development of a DNA e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                     Name  \\\n",
       "0     -1   2715              -1_study_thesis_model_based   \n",
       "1      0   1037           0_study_analysis_social_thesis   \n",
       "2      1    795             1_cells_cell_expression_role   \n",
       "3      2    707           2_model_numerical_study_method   \n",
       "4      3    634              3_data_thesis_systems_model   \n",
       "5      4    464  4_properties_magnetic_surface_synthesis   \n",
       "6      5    135         5_law_legal_international_rights   \n",
       "7      6     13   6_detection_biosensor_biosensors_based   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [study, thesis, model, based, analysis, data, ...   \n",
       "1  [study, analysis, social, thesis, french, poli...   \n",
       "2  [cells, cell, expression, role, species, study...   \n",
       "3  [model, numerical, study, method, experimental...   \n",
       "4  [data, thesis, systems, model, propose, based,...   \n",
       "5  [properties, magnetic, surface, synthesis, mat...   \n",
       "6  [law, legal, international, rights, european, ...   \n",
       "7  [detection, biosensor, biosensors, based, dna,...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [Inspired by the limitations of traditional PI...  \n",
       "1  [The impact study is an approach to support ch...  \n",
       "2  [The dendritic cells, principal antigen presen...  \n",
       "3  [Regardless the industry, additive manufacturi...  \n",
       "4  [Today, diseases and illnesses are becoming th...  \n",
       "5  [Magnetic nanoparticles are now used in a wide...  \n",
       "6  [This thesis revisits the classical doctrine o...  \n",
       "7  [This work presents the development of a DNA e...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_info_reduced = topic_model.get_topic_info()\n",
    "topic_info_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to before, the noise topic contains very generic keywords. As for the rest, we can identify seven[^10] main latent topics :\n",
    "\n",
    "- Social Sciences\n",
    "- Medicine and Health\n",
    "- Engineering Sciences, Experimentation and Simulation\n",
    "- Data analysis and Mathematics\n",
    "- Physics \n",
    "- Law, Finance and Policies\n",
    "- Biochemisty and sensors\n",
    " \n",
    "The topics are very general and give us key insights about our corpus. \n",
    "\n",
    "A detailed analysis can be useful to understand the structure of the corpus. For instance, the keywords for the second main topic are too general, but because we analysed our documents in details, we know that this topic is the result of merging \"1_mechanical_numerical_behavior\" and \"3_flow_numerical_fluid\", allowing us to name this topic \"Engineering Sciences, Experimentation and Simulation\".\n",
    "\n",
    "After reducing outliers, here is the distribution of topics across all out documents : \n",
    "\n",
    "[^10]: We have seven topics because we asked the topic model to reduce the number of topics to seven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Social Sciences</th>\n",
       "      <td>0</td>\n",
       "      <td>1749</td>\n",
       "      <td>27 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medicine and Health</th>\n",
       "      <td>1</td>\n",
       "      <td>1328</td>\n",
       "      <td>20 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engineering Sciences, Experimentation and Simulation</th>\n",
       "      <td>2</td>\n",
       "      <td>1106</td>\n",
       "      <td>17 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data analysis and Mathematics</th>\n",
       "      <td>3</td>\n",
       "      <td>1073</td>\n",
       "      <td>17 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Physics</th>\n",
       "      <td>4</td>\n",
       "      <td>803</td>\n",
       "      <td>12 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Law, Finance and Policies</th>\n",
       "      <td>5</td>\n",
       "      <td>410</td>\n",
       "      <td>6 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Biochemisty and sensors</th>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>0 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Topic  Count Proportion\n",
       "Social Sciences                                         0   1749       27 %\n",
       "Medicine and Health                                     1   1328       20 %\n",
       "Engineering Sciences, Experimentation and Simul...      2   1106       17 %\n",
       "Data analysis and Mathematics                           3   1073       17 %\n",
       "Physics                                                 4    803       12 %\n",
       "Law, Finance and Policies                               5    410        6 %\n",
       "Biochemisty and sensors                                 6     31        0 %"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_reduced_no_outliers = topic_model.reduce_outliers(\n",
    "    documents=docs, \n",
    "    topics = topics_reduced,\n",
    "    embeddings = embeddings,\n",
    "    probabilities = probabilities_reduced, \n",
    "    strategy = \"embeddings\"\n",
    ")\n",
    "unique, counts = np.unique(topics_reduced_no_outliers, return_counts=True)\n",
    "df_count = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"Topic\" : int(topic),\n",
    "            \"Count\" : int(count)\n",
    "        } \n",
    "        for topic, count in zip(unique, counts)\n",
    "    ]\n",
    ")\n",
    "df_count.index = [\n",
    "    \"Social Sciences\",\n",
    "    \"Medicine and Health\",\n",
    "    \"Engineering Sciences, Experimentation and Simulation\",\n",
    "    \"Data analysis and Mathematics\",\n",
    "    \"Physics \",\n",
    "    \"Law, Finance and Policies\",\n",
    "    \"Biochemisty and sensors\"\n",
    "]\n",
    "df_count[\"Proportion\"] = [f\"{100 * d:.0f} %\" for d in df_count[\"Count\"] / 6500]\n",
    "df_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the 7th topic still seems very specific, even after reducing outliers, there are only 31 documents in this topic.\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "You can merge topics by hand using the [`merge_topics` method](https://maartengr.github.io/BERTopic/api/bertopic.html#bertopic._bertopic.BERTopic.merge_topics).\n",
    "\n",
    "```python\n",
    "topics_to_merge = [1, 2, 3]\n",
    "topic_model.merge_topics(docs, topics_to_merge)\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune parameters{#sec-tune-parameters}\n",
    "\n",
    "A diversity of parameters can be used to tune the topic model. In this section, we propose to tune the 3 most useful parameters outlined in the litterature: the embedding model, `n_neighbors` (UMAP) and `n_components` (HDBSCAN). A table can be found in the [techy notes](https://css-polytechnique.github.io/css-ipp-materials/pages/techy-notes.html#umap-and-hdbscan-parameter-despcription) providing descriptions for other parameters.\n",
    "\n",
    "**1. Assess the quality of the text representations** (parameter tuned: embedding model)\n",
    "\n",
    "The primary factor to tune is the embedding model, because it drastically impacts the results of the topic model. To check if the embedding makes sense, you can plot the 2D map after dimension reduction with UMAP (`n_components=2`). Then, by exploring the map, you can assess if the embedding space created placed similar documents together or not.\n",
    "\n",
    "At this point you can also try different values for `n_neighbors` and `n_components`. However, be aware that the influence of UMAP parameters on the final topic model is difficult to appreciate at first glance.\n",
    "\n",
    "**Tune the granularity of the topic model** (parameters tuned: `n_neighbors` and `min_cluster_size`)\n",
    "\n",
    "Once you've chosen an embedding model, you can change the `n_neighbors` and `min_cluster_size`. Both work jointly: the lower these parameters, the smaller the grain and the more specific the topics. It is worth noting that these parameters are dependant of the size of your corpus. For a corpus of 5,000 documents, `n_neighbors=300` is a large value, but for 50,000 documents it might be a medium value.\n",
    "\n",
    "To change these parameters, one must explicitly declare `UMAP` and `HDBSCAN` objects and pass them on to the `BERTopic` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"english\" # or \"french\"\n",
    "language_short = language[:2] # \"en\" or \"fr\"\n",
    "ds = load_from_disk(f\"./data/embeddings/gte-multilingual-base-{language_short}-SBERT\")\n",
    "docs = np.array(ds[f\"resumes.{language_short}\"]) # 6500 rows\n",
    "embeddings = np.array(ds[\"embedding\"])\t\t\t # Shape : 6500 x 768\n",
    "vectorizer_model = CountVectorizer(stop_words = list(stopwords(language_short)))\n",
    "\n",
    "# create an HDBSCAN and UMAP models\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50, \n",
    "\t# Default parameters \n",
    "    prediction_data=True\n",
    ")\n",
    "umap_model = UMAP(\n",
    "    n_neighbors = 50,\n",
    "\t# Default parameters\n",
    "    metric = \"cosine\",\n",
    "    n_components = 5,\n",
    "    min_dist=0.0,\n",
    "    low_memory = False, \n",
    "    # For reproducibility \n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Then create the instance, fit the model and extract the topics and probabilities\n",
    "topic_model_coarse = BERTopic(\n",
    "\tlanguage = language,\n",
    "\tvectorizer_model = vectorizer_model,\n",
    "    umap_model= umap_model,\n",
    "    hdbscan_model=hdbscan_model\n",
    ")\n",
    "topics_coarse, probabilities_coarse = topic_model_coarse.fit_transform(\n",
    "    documents=docs, \n",
    "    embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>837</td>\n",
       "      <td>-1_thesis_model_study_data</td>\n",
       "      <td>[thesis, model, study, data, based, method, an...</td>\n",
       "      <td>[Forecasting of a physical system is computed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2917</td>\n",
       "      <td>0_study_model_cells_properties</td>\n",
       "      <td>[study, model, cells, properties, thesis, cell...</td>\n",
       "      <td>[Membrane separation processes are used on a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>1_study_social_thesis_law</td>\n",
       "      <td>[study, social, thesis, law, analysis, politic...</td>\n",
       "      <td>[Nature conservation strategies evolve from th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>735</td>\n",
       "      <td>2_data_thesis_systems_propose</td>\n",
       "      <td>[data, thesis, systems, propose, based, model,...</td>\n",
       "      <td>[The Internet of Things (IoT) and Smart Enviro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                            Name  \\\n",
       "0     -1    837      -1_thesis_model_study_data   \n",
       "1      0   2917  0_study_model_cells_properties   \n",
       "2      1   2011       1_study_social_thesis_law   \n",
       "3      2    735   2_data_thesis_systems_propose   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [thesis, model, study, data, based, method, an...   \n",
       "1  [study, model, cells, properties, thesis, cell...   \n",
       "2  [study, social, thesis, law, analysis, politic...   \n",
       "3  [data, thesis, systems, propose, based, model,...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [Forecasting of a physical system is computed ...  \n",
       "1  [Membrane separation processes are used on a l...  \n",
       "2  [Nature conservation strategies evolve from th...  \n",
       "3  [The Internet of Things (IoT) and Smart Enviro...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_coarse.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "\ttopic_model_coarse\n",
    "\t.visualize_documents(\n",
    "\t\tdocs = docs,\n",
    "\t\tembeddings = embeddings,\n",
    "\t\thide_annotations = True, # better readability\n",
    "\t\t# topics = [0,1,2,3],      # Select topics to highlight\n",
    "\t\theight = 600, # Adjust the height of the plot\n",
    "\t\twidth = 800 # Adjust the width of the plot \n",
    "\t)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_run cell to see the visualisation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional visualisations: cross-reference with additional tags\n",
    "\n",
    "If you have additional tags for your dataset, such as categories or dates, you can easily display your topic analysis in regard of these dimensions.\n",
    "\n",
    "In our case, we have OAI codes that account for the field each thesis is in. Hence we can compare the generated topics with the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some theses are in multiple fields, \n",
    "# the oai code is: \n",
    "# ddc:XXX||ddc:YYY\n",
    "# for simplicity, we are going to keep the first field for each thesis\n",
    "first_oai = [oai_code[:7] for oai_code in ds[\"oai_set_specs\"]]\n",
    "\n",
    "# Let's translate that to human language:\n",
    "oai_names = {\n",
    "    \"ddc:300\" : \"Sciences sociales, sociologie, anthropologie\",\n",
    "    \"ddc:340\" : \"Droit\",\n",
    "    \"ddc:004\" : \"Informatique\",\n",
    "    \"ddc:570\" : \"Sciences de la vie, biologie, biochimie\",\n",
    "    \"ddc:540\" : \"Chimie, minéralogie, cristallographie\",\n",
    "    \"ddc:620\" : \"Sciences de l'ingénieur\",\n",
    "    \"ddc:550\" : \"Sciences de la terre\",\n",
    "    \"ddc:530\" : \"Physique\",\n",
    "    \"ddc:510\" : \"Mathématiques\",\n",
    "    \"ddc:610\" : \"Médecine et santé\"\n",
    "}\n",
    "def retrieve_name(oai_code):\n",
    "    if oai_code in oai_names:\n",
    "        return oai_names[oai_code]\n",
    "    else : \n",
    "        return \"Autre\"\n",
    "\n",
    "first_oai_names = [retrieve_name(oai_code) for oai_code in first_oai]\n",
    "\n",
    "topics_per_class = topic_model.topics_per_class(docs, classes=first_oai_names)\n",
    "topic_model.visualize_topics_per_class(\n",
    "    topics_per_class,\n",
    "    topics = [0, 1, 2, 3], # choose specifically which topics to display\n",
    "    # top_n_topics = 10,   # choose to display the 10 largest topics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_run cell to see the visualisation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this figure, we can start by checking that documents are associated to the right topic. For instance, it is coherent to see that documents of the topic '1_cells_cell_expression_role' are found in theses in Health and Medicine as well as in Biology and Biochemistry and that documents of the topic '2_model_numerical_study_method' are found in theses in Physics and Engineering science, . \n",
    "\n",
    "If you want to visualise your topics on a temporal axis, you can use the the `visualize_topics_over_time` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = [int(float(year_as_string)) for year_as_string in ds[\"year\"]]\n",
    "topics_over_time = topic_model.topics_over_time(docs = docs, timestamps=year)\n",
    "topic_model.visualize_topics_over_time(topics_over_time, topics = [0,1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_run cell to see the visualisation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your topic model\n",
    "\n",
    "Topic model evaluation is an active domain of research that goes beyond the scope of this tutorial. We propose an overview of the methods that exist and how to quickly tell if your topic model can be used or needs to be refined.\n",
    "\n",
    "In short: quantitative methods are impractical and one should focus more on the qualitative evaluation.\n",
    "\n",
    "## Qualitative evaluation\n",
    "\n",
    "Throughout this tutorial, we have displayed many results and analysed them with the objective to answer the very question: is the topic model consistent? There is no one way around qualitatively evaluating your BERTopic, however the point is to offer some techniques we found useful and code snippets to quickly obtain key insights on your topic model performance.\n",
    "\n",
    "**Extensively use the visualisation tools**\n",
    "\n",
    "As presented in section 3.4 and 4.3, there are many tools to visualise the results of your topic model that can help you assess the coherence of the topics through 2D representation, top n words, hierarchical and the tag distribution. \n",
    "\n",
    "**Explore the merging process**\n",
    "\n",
    "When merging topics at section 4.1 we may want to monitor what's going where. The following snippet prints out the topics that were merged into them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 study thesis model based\n",
      "\t -  -1 study thesis model based\n",
      "---\n",
      "0 study analysis social thesis\n",
      "\t -  0 literary writing art contemporary\n",
      "\t -  5 urban planning land city\n",
      "\t -  8 archaeological sites funerary century\n",
      "\t -  14 innovation organizational management customer\n",
      "\t -  15 teachers education teaching school\n",
      "\t -  17 century royal kingdom king\n",
      "\t -  19 migration migrants immigration social\n",
      "\t -  30 english french l2 lexical\n",
      "\t -  32 language languages french students\n",
      "\t -  36 adolescents violence children parents\n",
      "\t -  42 infants motor perception preterm\n",
      "\t -  43 war diplomacy french political\n",
      "\t -  50 psychological health organizational absenteeism\n",
      "\t -  54 museums museum tourism wildlife\n",
      "\t -  55 turkey political muslim despotism\n",
      "\t -  60 athletes performance fatigue adults\n",
      "\t -  74 patients copd tracheal cuff\n",
      "\t -  75 cancer breast women sds\n",
      "\t -  80 maternal breastfeeding antenatal smm\n",
      "\t -  84 queer gender women sexual\n",
      "\t -  87 game games video gaming\n",
      "\t -  91 literary mystical attajalliyt arabic\n",
      "\t -  92 digital political internet online\n",
      "\t -  94 wage employment wages labour\n",
      "\t -  96 depression patients rnt dd\n",
      "---\n",
      "1 cells cell expression role\n",
      "\t -  2 cells cancer tumor cell\n",
      "\t -  4 bacteria strains bacterial resistance\n",
      "\t -  11 species diversity pest populations\n",
      "\t -  12 dna transcription chromatin genes\n",
      "\t -  18 phytoplankton species coral ocean\n",
      "\t -  24 neurons receptors fear memory\n",
      "\t -  39 microbiota intestinal mice insulin\n",
      "\t -  41 bone cells stem mscs\n",
      "\t -  48 hiv1 viral gag infection\n",
      "\t -  49 mri liver mre imaging\n",
      "\t -  57 tau amyloid microglial deficits\n",
      "\t -  59 male sperm testis lxr\n",
      "\t -  62 tumor imaging cells glioblastoma\n",
      "\t -  63 channels osk52 membrane cell\n",
      "\t -  65 sensory compounds wines nutritional\n",
      "\t -  68 kidney cmip wt1 renal\n",
      "\t -  76 production hemicelluloses 3hp biomass\n",
      "\t -  77 dyrk1a mitochondrial nf1 patients\n",
      "\t -  90 rna virus viral polymerase\n",
      "\t -  93 basal embryo apical fopnl\n",
      "\t -  101 influenza vaccination meningitis virus\n",
      "\t -  102 ablation atrial intracranial vein\n",
      "---\n",
      "2 model numerical study method\n",
      "\t -  1 mechanical numerical behavior material\n",
      "\t -  3 flow numerical fluid acoustic\n",
      "\t -  21 crust sedimentary sediment basin\n",
      "\t -  25 laser lasers optical quantum\n",
      "\t -  27 climate rainfall cloud assimilation\n",
      "\t -  40 voltage gate devices noise\n",
      "\t -  45 seismic waves volcanic landslides\n",
      "\t -  51 stellar stars planets star\n",
      "\t -  61 boson higgs atlas lhc\n",
      "\t -  64 vibrational molecule spectra spectroscopy\n",
      "\t -  69 wood drying binder log\n",
      "\t -  73 heat temperature thermal gas\n",
      "\t -  78 plasma flux tokamak quench\n",
      "\t -  79 dielectric charge permittivity electric\n",
      "\t -  81 arc dc voltage electrical\n",
      "\t -  83 holes cosmological black gravitational\n",
      "\t -  86 fiber optical frequency transmission\n",
      "\t -  97 longrange pairing nuclei decay\n",
      "\t -  99 groundwater irrigation aquifer recharge\n",
      "\t -  103 building ventilation thermal heat\n",
      "---\n",
      "3 data thesis systems model\n",
      "\t -  33 volatility financial market price\n",
      "\t -  34 equation control prove boundary\n",
      "\t -  98 sound hrtf musical music\n",
      "\t -  37 classification data learning series\n",
      "\t -  38 haptic knee walking bone\n",
      "\t -  9 robot camera gps detection\n",
      "\t -  10 network networks wireless nodes\n",
      "\t -  44 estimator estimators random convergence\n",
      "\t -  13 systems software verification security\n",
      "\t -  20 graphs graph theorem prove\n",
      "\t -  52 biological docking data networks\n",
      "\t -  53 scheduling production maintenance planning\n",
      "\t -  56 color image rendering images\n",
      "\t -  26 security privacy iot cryptographic\n",
      "\t -  29 computing data execution systems\n",
      "\t -  95 hevi estimation timedelay stochastic\n",
      "\t -  31 ontology semantic knowledge annotation\n",
      "---\n",
      "4 properties magnetic surface synthesis\n",
      "\t -  35 catalysts catalyst catalytic reaction\n",
      "\t -  100 xe adsorption zeolite diffusion\n",
      "\t -  67 polymerization catalysts complexes catalyst\n",
      "\t -  6 synthesis complexes reaction reactions\n",
      "\t -  7 silicon optical solar nanowires\n",
      "\t -  104 humins tannins biobased foams\n",
      "\t -  71 corrosion alloy alloys surface\n",
      "\t -  72 nanoparticles nps magnetic eu3\n",
      "\t -  47 electrochemical oxides electrode batteries\n",
      "\t -  16 magnetic spin magnetization temperature\n",
      "\t -  85 cnt nanotubes cnts carbon\n",
      "\t -  23 biofilm biofilms microgels emulsions\n",
      "\t -  88 photocatalytic tio2 degradation photocatalysis\n",
      "\t -  28 water membrane desalination polymer\n",
      "---\n",
      "5 law legal international rights\n",
      "\t -  66 monetary policy inflation exchange\n",
      "\t -  70 contract law contractual subsidy\n",
      "\t -  46 constitutional judge legal council\n",
      "\t -  22 international law european rights\n",
      "\t -  89 financial law tax transactions\n",
      "\t -  58 financial economic banks growth\n",
      "---\n",
      "6 detection biosensor biosensors based\n",
      "\t -  82 biosensor detection biosensors sers\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for iRow_reduced, topic_id in enumerate(topic_info_reduced[\"Topic\"]):\n",
    "    print(topic_info_reduced.loc[iRow_reduced, \"Name\"].replace(\"_\", \" \"))\n",
    "    og_topics_merged_to_new_topic = list(set([\n",
    "        int(og_topic) \n",
    "        for og_topic, new_topic in zip(topics, topics_reduced) \n",
    "        if new_topic == topic_id\n",
    "    ]))\n",
    "    for og_topic in og_topics_merged_to_new_topic:\n",
    "        print(\n",
    "            \"\\t - \",\n",
    "            topic_info.loc[\n",
    "                topic_info[\"Topic\"] == og_topic,\n",
    "                \"Name\"\n",
    "            ]\n",
    "            .item()\n",
    "            .replace(\"_\", \" \")\n",
    "        ) \n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we can confidently assess that the merge is coherent as it groups all law related topics into one.\n",
    "\n",
    "**Explore the reason why a given document was clustered in a specific group**\n",
    "\n",
    "The `topics_per_class` is a powerful method that retrieves top keywords found in specific documents that justify assigning it to a given cluster: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc n°3000:\n",
      "The overall objective of this thesis is to exploit a user centric means of representation, acquisition, enrichment and exploitation of multimedia document metadata. To achieve this goal, we proposed an annotation model, called SeMAT with a new vision of the snapshot context. We proposed the usage of external semantic resources (e.g. GeoNames ,, Wikipedia , etc.) to enrich the annotations automatically from the snapshot contextual elements. To accentuate the annotations semantic aspect, we modeled the concept of ‘social profile’ with Semantic web tools by focusing, in particular, on social relationships and a reasoning mechanism to infer a non-explicit social relationship. The proposed model, called SocialSphere is aimed to exploit a way to personalize the annotations to the viewer. Examples can be user’s objects (e.g. home, work) or user’s social dimensions (e.g. my mother, my husband's cousin). In this context, we proposed an algorithm, called SQO to suggest social dimensions describing actors in multimedia documents according to the viewer’s social profile. For suggesting event annotations, we have reused user experience and the experience of the users in his social network by producing association rules. In the last part, we addressed the problem of pattern matching between query and social graph. We proposed to steer the problem of pattern matching to a sub-graph isomorphism problem. We proposed an algorithm, called h-Pruning, for partial sub-graph isomorphism to ensure a close matching between nodes of the two graphs: motive (representing the request) and the social one. For implementation, we realized a prototype having two components: mobile and web. The mobile component aims to capture the snapshot contextual elements. As for the web component, it is dedicated to the assistance of the user during his socio-personnel multimedia document annotation or socio-personnel multimedia document consultation. The evaluation have proven: (i) the effectiveness of our exploitation of social graph approach in terms of execution time, (ii) the effectiveness of our event suggestion approach (we proved our hypothesis by demonstrating the existence of co-occurrence between the spatio-temporal context and events), (iii) the effectiveness of our social dimension suggestion approach in terms of execution time.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Class</th>\n",
       "      <th>Topic Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>social, annotations, multimedia, snapshot, pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3_data_thesis_systems_model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Words  Frequency  Class  \\\n",
       "Topic                                                                        \n",
       "3      social, annotations, multimedia, snapshot, pro...          1   True   \n",
       "\n",
       "                        Topic Name  \n",
       "Topic                               \n",
       "3      3_data_thesis_systems_model  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a document\n",
    "text_id = 3000\n",
    "is_my_document = [i == text_id for i in range(len(docs))]\n",
    "print(f\"Doc n°{text_id}:\\n{docs[text_id]}\")\n",
    "\n",
    "topics_per_class = topic_model.topics_per_class(docs, classes = is_my_document)\n",
    "topics_per_class = topics_per_class.loc[topics_per_class[\"Class\"], :].set_index(\"Topic\")\n",
    "# Retrieve the Topic Representation for comparison\n",
    "topics_name = (topic_model.get_topic_info().set_index(\"Topic\")[\"Name\"])\n",
    "topics_per_class.loc[:,\"Topic Name\"] = topics_name\n",
    "topics_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have additional tags, it's even more powerful as you can check the keywords for a whole class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Words</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Class</th>\n",
       "      <th>Topic Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>social, thesis, study, public, political</td>\n",
       "      <td>98</td>\n",
       "      <td>Sciences sociales, sociologie, anthropologie</td>\n",
       "      <td>-1_study_thesis_model_based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>social, study, thesis, french, analysis</td>\n",
       "      <td>73</td>\n",
       "      <td>Sciences sociales, sociologie, anthropologie</td>\n",
       "      <td>0_study_analysis_social_thesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>markov, volatility, smc, stochastic, em</td>\n",
       "      <td>1</td>\n",
       "      <td>Sciences sociales, sociologie, anthropologie</td>\n",
       "      <td>3_data_thesis_systems_model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic                                     Words  Frequency  \\\n",
       "0     -1  social, thesis, study, public, political         98   \n",
       "1      0   social, study, thesis, french, analysis         73   \n",
       "2      3   markov, volatility, smc, stochastic, em          1   \n",
       "\n",
       "                                          Class  \\\n",
       "0  Sciences sociales, sociologie, anthropologie   \n",
       "1  Sciences sociales, sociologie, anthropologie   \n",
       "2  Sciences sociales, sociologie, anthropologie   \n",
       "\n",
       "                       Topic Name  \n",
       "0     -1_study_thesis_model_based  \n",
       "1  0_study_analysis_social_thesis  \n",
       "2     3_data_thesis_systems_model  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OAI_REFS = pd.read_csv(\"./data/oai_codes.csv\")\n",
    "DS = ds\n",
    "def get_docs_for_oai_code(oai_code : str):\n",
    "    try: \n",
    "        oai_name = OAI_REFS.loc[OAI_REFS[\"code\"] == oai_code, \"name\"].item()\n",
    "    except Exception:\n",
    "        print(f\"oai code {oai_code} invalid\\n\\nException:\\n{Exception}\")\n",
    "        return\n",
    "    \n",
    "    def return_name(codes):\n",
    "        if oai_code in codes: \n",
    "            return oai_name\n",
    "        else: \n",
    "            return \"Autre\"\n",
    "\n",
    "    classes = [return_name(codes) for codes in ds[\"oai_set_specs\"]]\n",
    "    topics_per_class = topic_model.topics_per_class(docs, classes = classes)\n",
    "    topics_per_class = topics_per_class.loc[topics_per_class[\"Class\"] == oai_name, :].set_index(\"Topic\")\n",
    "    topics_name = (\n",
    "        topic_model\n",
    "        .get_topic_info()\n",
    "        .set_index(\"Topic\")\n",
    "        [\"Name\"]\n",
    "    )\n",
    "    topics_per_class.loc[:,\"Topic Name\"] = topics_name\n",
    "    return topics_per_class.loc[topics_per_class[\"Class\"] == oai_name, :].reset_index()\n",
    "\n",
    "get_docs_for_oai_code(\"ddc:300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative evaluation\n",
    "\n",
    "In this section we introduce different metrics that can be used to evaluate your topic model. However, we mainly included it to warn you of the complexity behind evaluating a topic model and that there is no one-size-fit-all solution. \n",
    "\n",
    "> Response to \"How to evaluate the performance of the model?\" by Maarten Grootendorst [source](https://github.com/MaartenGr/BERTopic/issues/437)\n",
    ">\n",
    "> First, choosing the coherence score by itself can have a large influence on the difference in performance you will find between models. For example, NPMI and UCI may each lead to quite different values. Second, the coherence score only tells a part of the story. Perhaps your purpose is more classification than having the most coherent words or perhaps you want as diverse topics as possible. These use cases require vastly different evaluation metrics to be used.\n",
    "\n",
    "There are two types of metrics that you could use:\n",
    "\n",
    "- Cluster metrics — ie focus on the group-making. There exist a lot of metrics, but few are fit to our situation: unsupervised learning with density based algorithms. In our experience, optimising these metrics results in a sub-optimal solutions as illustrated bellow. [Read more](./techy-notes.qmd#clustering-metrics)\n",
    "- Topic representation metrics — ie focus on how relevant the keywords are. Although some metrics exist their utility is limited: good score does not necessarily align with what expert consider good topic models, and they are not good scores to optimise (Stammbach et al., 2023). [Read more](./techy-notes.qmd#topic-representation-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some good practices\n",
    "\n",
    "Now that you have a good understanding of BERTopic, and you started to experiment with it, you may want more practical advices. Here, we list some tips to reduce computation time and facilitate reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your instance locally\n",
    "\n",
    "For reproducibility purposes, BERTopic lets you save the BERTopic object you created with the `save`[^25] method. Two parameters of importance:\n",
    "\n",
    "- `serialization (str)`: must be `\"safetensors\"`, `\"pickle\"` or `\"pytorch\"`. We recommend using `\"safetensors\"` or the `\"pytorch\"` format as they are broadly used in machine learning and recommended by the [BERTopic documentation](https://maartengr.github.io/BERTopic/api/bertopic.html#bertopic._bertopic.BERTopic.save).\n",
    "- `save_ctfidf (bool)` : wether to save the vectorizer configuration or not. This is the heaviest bit (see table below).\n",
    "\n",
    "```python\n",
    "# ~ 500 KB\n",
    "topic_model.save(\n",
    "\tpath = \"./bertopic-default\",\n",
    "\tserialization = \"safetensors\",\n",
    "\tsave_ctfidf = False\n",
    ")\n",
    "\n",
    "# ~ 6MB\n",
    "topic_model.save(\n",
    "\tpath = \"./bertopic-default-with-ctfidf\",\n",
    "\tserialization = \"safetensors\",\n",
    "\tsave_ctfidf = True\n",
    ")\n",
    "```\n",
    "\n",
    "To reload your instance you just need to use the `load` method:\n",
    "\n",
    "```python\n",
    "topic_model = BERTopic.load(\"./bertopic-default\")\n",
    "```\n",
    "\n",
    "Saving the instance is a good practice, as we will see below, when reducing the number of topics, the instance is updated and you can't go back. Hence, we would recommend to save at least one instance — _or rerun the whole cell_.\n",
    "\n",
    "## Precompute your embeddings\n",
    "\n",
    "Pre-computing the embeddings is a good practice as it will prevent from computing them at each run, but also because it allows you to use a broader spectrum of embedding models. This comes handy when you want to test different parameters of clustering and cluster representation. Moreover, saving BERTopics models does not save the embeddings, so it is good practice to manage them separately.\n",
    "\n",
    "To embed our documents, we use the [datasets](https://huggingface.co/docs/datasets/index) objects to manage the data and the [sentence-bert](https://www.sbert.net) (**SBERT**) library to embed the documents. The process is very straightforward, you need to open your file and preprocess your texts. Then after loading the model \n",
    "\n",
    "```python\n",
    "from datasets import Dataset\n",
    "from gc import collect as gc_collect\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.cuda import is_available as cuda_available\n",
    "from torch.cuda import synchronize, ipc_collect, empty_cache\n",
    "\n",
    "ds = Dataset.load_from_disk(\"...\")\n",
    "# implement your preprocess and open functions\n",
    "texts : list[str] = preprocess(ds[\"texts\"]) \n",
    "# Use GPU if you have one\n",
    "device = \"cuda\" if cuda_available() else \"cpu\" \n",
    "\n",
    "sbert_model = SentenceTransformer(\n",
    "    model_name, \n",
    "    device = device, \n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "sbert_model.max_seq_length = min(\n",
    "    sbert_model.max_seq_length,\n",
    "    np.inf # Replace with desired window size\n",
    ")\n",
    "\n",
    "try : \n",
    "    embeddings = (\n",
    "        sbert_model\n",
    "        .encode(\n",
    "            texts, \n",
    "            device=str(device), \n",
    "            normalize_embeddings=True, \n",
    "            show_progress_bar=True\n",
    "        )\n",
    "    )\n",
    "    ds = ds.add_column(\"embedding\", list(embeddings))\n",
    "    ds.save_to_disk(\"embeddings\")\n",
    "except Exception:\n",
    "    print(Exception)\n",
    "finally:\n",
    "    # Make sure to clean your GPU\n",
    "    del sbert_model, ds\n",
    "    empty_cache()\n",
    "    if cuda_available():\n",
    "        synchronize()\n",
    "        ipc_collect()\n",
    "    gc_collect()\n",
    "\n",
    "```\n",
    "\n",
    "We retrieve the embeddings and the documents\n",
    "\n",
    "```python\n",
    "ds = load_from_disk(\"path/to/file\")\n",
    "docs = np.array(ds[f\"texts\"]) # Number of documents : 6500\n",
    "embeddings = np.array(ds[\"embedding\"]) # shape : (6500, 768)\n",
    "```\n",
    "\n",
    "## Force deterministic behaviour \n",
    "\n",
    "The BERTopic pipeline is deterministic apart from the UMAP component. To force a deterministic behaviour: \n",
    "\n",
    "```python \n",
    "topic_model = BERTopic(\n",
    "    ...\n",
    "    umap_model= UMAP(\n",
    "        ...\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "You can also set the random state for Numpy (used by Pandas) with `np.random.seed(RANDOM_SEED)`.\n",
    "\n",
    "[^25]: See [techy notes](https://css-polytechnique.github.io/css-ipp-materials/pages/techy-notes.html#instances-file-size-and-content) for more information on what's saved and the size of the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limits of BERTopic and topic modelling in general\n",
    "\n",
    "Despite the good results we have demonstrated in this tutorial, BERTopic faces some limits. In this section we try to summarise them and highlight valuable resources if you want to investigate.\n",
    "\n",
    "The first limit BERTopic faces is that it assumes that one document fits in only one category. This assumption may flattend the corpus' complexity; this can, in theory, be mitigated by using HDBSCAN probability matrix to assign multiple topics to one document (Grootendorst, 2022 §7.2)[^27]. On top of that, results can be very dependant of the task and parameters requiring extra tuning and validation time. It is often advised to try different topic model techniques to cross reference your results.\n",
    "<br/>\n",
    "When comparing BERTopic with LDA, some experiments report BERTopic underperforming (Hoyle et al., 2025; Li et al., 2025) while others highlight BERTopic's capability to highlight different insightful dimensions of their corpus (Egger & Yu, 2022; Ma et al., 2025). These remarks highlight that NLP techniques and pipelines are heavily task-dependant (Egami et al., 2024; Ollion et al., 2023). These remarks further stress the point made in section 5: the only evaluation that must dictate your choice of method, model and parameters is the qualitative evaluation by experts.\n",
    "\n",
    "Topic models in general also suffer linguistic limitations (Shadrova, 2021)[^28]. From a linguistic perspective, these methods lack conceptualisation and therefore, are difficult to validate and utilise. Other critics center around the interpretability of the results and the overall difficulty to fully validate a topic model.\n",
    "\n",
    "[^27]: We have not found resources to do so.\n",
    "\n",
    "[^28]: The cited paper addresses topic models' limitations in general but use BoW techniques as a starting point for their analysis (namely LSI, LSA, NMF and LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this tutorial we have explained how to use BERTopic a Python library that facilitates the exploration of a corpus of text. The pipeline leverages several NLP tools such as encoder models and clustering techniques to generate groups of similar texts, as well as bag-of-words techniques to retrieve insightful keywords. We have demonstrated how to create a topic model, tune it and visualise the results. We have also provided ready-to-use techniques to qualitatively evaluate your topic model. \n",
    "\n",
    "The most important steps to follow to obtain a coherent topic model are : \n",
    "\n",
    "- Define what you want out of your topic model and preprocess your texts accordingly;\n",
    "- Carefully choose your embedding model and assess the quality of the embedding space;\n",
    "- Tune your parameters in order to get a topic model with the desired granularity;\n",
    "- Visualise your results and assess the quality of your topics and the coherence of the documents within a given topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "Asmussen, C. B., & Møller, C. (2019). Smart literature review : A practical topic modelling approach to exploratory literature review. Journal of Big Data, 6(1), 93. https://doi.org/10.1186/s40537-019-0255-7\n",
    "\n",
    "Bizel-Bizellot, G., Galmiche, S., Charmet, T., Coudeville, L., Fontanet, A., & Zimmer, C. (2024). Extracting Circumstances of COVID-19 Transmission from Free Text with Large Language Models. SSRN. https://doi.org/10.2139/ssrn.4819301\n",
    "\n",
    "DiMaggio, P., Nag, M., & Blei, D. (2013). Exploiting affinities between topic modeling and the sociological perspective on culture : Application to newspaper coverage of U.S. government arts funding. Poetics, 41(6), 570‑606. https://doi.org/10.1016/j.poetic.2013.08.004\n",
    "\n",
    "Egami, N., Hinck, M., Stewart, B. M., & Wei, H. (2024). Using Large Language Model Annotations for the Social Sciences : A General Framework of Using Predicted Variables in Downstream Analyses.\n",
    "\n",
    "Grootendorst, M. (2022). BERTopic : Neural topic modeling with a class-based TF-IDF procedure (Version 1). arXiv. https://doi.org/10.48550/ARXIV.2203.05794\n",
    "\n",
    "Hoyle, A., Calvo-Bartolomé, L., Boyd-Graber, J., & Resnik, P. (2025). PROXANN: Use-Oriented Evaluations of Topic Models and Document Clustering.\n",
    "\n",
    "Jockers, M. L., & Mimno, D. (2013). Significant themes in 19th-century literature. Poetics, 41(6), 750‑769. https://doi.org/10.1016/j.poetic.2013.08.005\n",
    "\n",
    "Li, Z., Calvo-Bartolomé, L., Hoyle, A., Xu, P., Stephens, D., Dima, A., Fung, J. F., & Boyd-Graber, J. (2025). Large Language Models Struggle to Describe the Haystack without Human Help : A Social Science-Inspired Evaluation of Topic Models.\n",
    "\n",
    "Ma, L., Chen, R., Ge, W., Rogers, P., Lyn-Cook, B., Hong, H., Tong, W., Wu, N., & Zou, W. (2025). AI-powered topic modeling : Comparing LDA and BERTopic in analyzing opioid-related cardiovascular risks in women. Experimental Biology and Medicine, 250, 10389. https://doi.org/10.3389/ebm.2025.10389\n",
    "\n",
    "McInnes, L., Healy, J., & Melville, J. (2018). UMAP : Uniform Manifold Approximation and Projection for Dimension Reduction (Version 3). arXiv. https://doi.org/10.48550/ARXIV.1802.03426\n",
    "\n",
    "Ollion, E., Shen, R., Macanovic, A., & Chatelain, A. (2023). ChatGPT for Text Annotation? Mind the Hype! SocArXiv. https://doi.org/10.31235/osf.io/x58kn\n",
    "\n",
    "Ollion, E., Boelaert, J., Coavoux, S., Delaine, E., Desprès, A., Gollac, S., Keyhani, N., & Mommeja, A. (2025). La part du genre. Genre et approche intersectionnelle dans les sciences sociales françaises au XXIe siècle. https://doi.org/10.31235/osf.io/qamux_v2\n",
    "\n",
    "Shadrova, A. (2021). Topic models do not model topics : Epistemological remarks and steps towards best practices. Journal of Data Mining & Digital Humanities, 2021, 7595. https://doi.org/10.46298/jdmdh.7595\n",
    "\n",
    "Stammbach, D., Zouhar, V., Hoyle, A., Sachan, M., & Ash, E. (2023). Revisiting Automated Topic Model Evaluation with Large Language Models. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 9348‑9357. https://doi.org/10.18653/v1/2023.emnlp-main.581\n",
    "\n",
    "Törnberg, A., & Törnberg, P. (2025). The aesthetics of climate misinformation : Computational multimodal framing analysis with BERTopic and CLIP. Environmental Politics, 1‑24. https://doi.org/10.1080/09644016.2025.2557684"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
